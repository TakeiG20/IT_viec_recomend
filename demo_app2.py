import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import re
import string
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, silhouette_score
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity

try:
    from pyspark.sql import SparkSession
    from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType
    from pyspark.ml.feature import VectorAssembler, StringIndexer
    from pyspark.ml.classification import LogisticRegression as SparkLogisticRegression, DecisionTreeClassifier as SparkDecisionTreeClassifier
    from pyspark.ml.evaluation import MulticlassClassificationEvaluator
    from pyspark.ml import Pipeline
    PYSPARK_AVAILABLE = True
except ImportError:
    PYSPARK_AVAILABLE = False
    st.warning("‚ö†Ô∏è PySpark kh√¥ng ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ch·ªâ s·ª≠ d·ª•ng sklearn models.")

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="AI Company Recommendation System AT IT_Viec",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS t√πy ch·ªânh
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .section-header {
        font-size: 1.8rem;
        font-weight: bold;
        color: #2c3e50;
        margin-bottom: 1rem;
    }
    .company-card {
        background-color: #f8f9fa;
        padding: 1rem;
        border-radius: 10px;
        border-left: 4px solid #1f77b4;
        margin-bottom: 1rem;
    }
    .creator-info {
        background-color: #e8f4f8;
        padding: 1rem;
        border-radius: 8px;
        margin-top: 2rem;
    }
    .prediction-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 15px;
        color: white;
        margin-bottom: 2rem;
    }
    .similarity-score {
        background-color: #e3f2fd;
        padding: 0.5rem;
        border-radius: 5px;
        font-weight: bold;
        color: #1976d2;
    }
    .ai-mode {
        background: linear-gradient(45deg, #FF6B6B, #4ECDC4);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        margin-bottom: 1rem;
        text-align: center;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        color: #856404;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# Kh·ªüi t·∫°o session state
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
if 'model_trained' not in st.session_state:
    st.session_state.model_trained = False

# Class x·ª≠ l√Ω vƒÉn b·∫£n theo code c·ªßa b·∫°n
class TextProcessor:
    def __init__(self):
        self.teen_dict = {}
        self.stopwords_lst = []
        self.wrong_lst = []
        self.load_dictionaries()
    
    def load_dictionaries(self):
        """Load c√°c t·ª´ ƒëi·ªÉn x·ª≠ l√Ω vƒÉn b·∫£n"""
        try:
            # Load teencode
            if os.path.exists('files/teencode.txt'):
                with open('files/teencode.txt', 'r', encoding="utf8") as file:
                    teen_lst = file.read().split('\n')
                    for line in teen_lst:
                        if '\t' in line:
                            key, value = line.split('\t', 1)
                            self.teen_dict[key] = str(value)
            
            # Load stopwords
            if os.path.exists('files/vietnamese-stopwords_rev.txt'):
                with open('files/vietnamese-stopwords_rev.txt', 'r', encoding="utf8") as file:
                    self.stopwords_lst = file.read().split('\n')
            
            # Load wrong words
            if os.path.exists('files/wrong-word_rev.txt'):
                with open('files/wrong-word_rev.txt', 'r', encoding="utf8") as file:
                    self.wrong_lst = file.read().split('\n')
                    
        except Exception as e:
            st.warning(f"Kh√¥ng th·ªÉ load m·ªôt s·ªë file t·ª´ ƒëi·ªÉn: {e}")
    
    def clean_text(self, text):
        """L√†m s·∫°ch vƒÉn b·∫£n c∆° b·∫£n"""
        if not text or pd.isna(text):
            return ""
        text = str(text).lower()
        text = re.sub(rf"[{string.punctuation}]", "", text)
        return text
    
    def fix_teencode(self, text):
        """S·ª≠a teencode"""
        words = text.split()
        corrected = [self.teen_dict.get(word, word) for word in words]
        return " ".join(corrected)
    
    def remove_wrongword(self, text):
        """Lo·∫°i b·ªè t·ª´ sai"""
        words = text.split()
        trueword = [word for word in words if word not in self.wrong_lst]
        return " ".join(trueword)
    
    def remove_stopword(self, text):
        """Lo·∫°i b·ªè stopwords"""
        words = text.split()
        stopword = [word for word in words if word not in self.stopwords_lst]
        return " ".join(stopword)
    
    def clean_pipeline(self, text):
        """Pipeline x·ª≠ l√Ω vƒÉn b·∫£n ho√†n ch·ªânh theo code c·ªßa b·∫°n"""
        if not text or pd.isna(text):
            return ""
        text = self.clean_text(text)
        text = self.fix_teencode(text)
        text = self.remove_wrongword(text)
        text = self.remove_stopword(text)
        return text

# Class h·ªá th·ªëng ML theo code c·ªßa b·∫°n
class CompanyRecommendationSystem:
    def __init__(self):
        self.clustercty = None
        self.tfidf = TfidfVectorizer(max_features=500)
        self.pca = PCA(n_components=50, random_state=42)
        self.best_model = None
        self.df_structured_encoded = None
        self.X_full = None
        self.text_processor = TextProcessor()
        self.structured_cols = ['Company Type', 'Company industry', 'Company size', 'Country', 'Working days', 'Overtime Policy']
        self.text_cols = ['Company overview_new', "Why you'll love working here_new", 'Our key skills_new', 'keyword']
        
    @st.cache_data
    def load_data(_self):
        """Load v√† x·ª≠ l√Ω d·ªØ li·ªáu theo code c·ªßa b·∫°n"""
        try:
            # ƒê∆∞·ªùng d·∫´n file - c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh theo c·∫•u tr√∫c th∆∞ m·ª•c c·ªßa b·∫°n
            data_paths = {
                'translated_data': 'data/translated_data.csv',
                'top2_clusters': 'data/top2_clusters_per_company.csv', 
                'sentiment_data': 'data/sentiment_by_company.csv'
            }
            
            # Ki·ªÉm tra file t·ªìn t·∫°i
            missing_files = []
            for name, path in data_paths.items():
                if not os.path.exists(path):
                    missing_files.append(path)
            
            if missing_files:
                st.error(f"Kh√¥ng t√¨m th·∫•y c√°c file: {missing_files}")
                return False
            
            # Load d·ªØ li·ªáu theo code c·ªßa b·∫°n
            clustercty = pd.read_csv(data_paths['translated_data'])
            new_data = pd.read_csv(data_paths['top2_clusters'])
            sentiment_cln = pd.read_csv(data_paths['sentiment_data'])
            
            # Merge d·ªØ li·ªáu theo code c·ªßa b·∫°n
            new_data = pd.merge(new_data, sentiment_cln[['Company Name','sentiment_group']], on='Company Name', how='left')
            clustercty = clustercty.merge(new_data[['Company Name', 'keyword', 'sentiment_group']], on='Company Name', how='left')
            
            # X·ª≠ l√Ω c·ªôt kh√¥ng c·∫ßn thi·∫øt
            if 'Unnamed: 0' in clustercty.columns:
                clustercty.drop(columns=['Unnamed: 0'], inplace=True)
            
            # ƒêi·ªÅn gi√° tr·ªã null
            clustercty['keyword'].fillna('kh√¥ng x√°c ƒë·ªãnh', inplace=True)
            clustercty['sentiment_group'].fillna('neutral', inplace=True)
            
            _self.clustercty = clustercty
            return True
            
        except Exception as e:
            st.error(f"L·ªói load d·ªØ li·ªáu: {e}")
            return False
    
    def prepare_features(self):
        """Chu·∫©n b·ªã features theo code c·ªßa b·∫°n"""
        try:
            # X·ª≠ l√Ω vƒÉn b·∫£n theo code c·ªßa b·∫°n
            self.clustercty['combined_text'] = self.clustercty[self.text_cols].fillna('').agg(' '.join, axis=1)
            self.clustercty['combined_text'] = self.clustercty['combined_text'].apply(self.text_processor.clean_pipeline)
            
            # TF-IDF theo code c·ªßa b·∫°n
            df_tfidf = pd.DataFrame(
                self.tfidf.fit_transform(self.clustercty['combined_text']).toarray(),
                columns=self.tfidf.get_feature_names_out()
            )
            
            # One-hot encode theo code c·ªßa b·∫°n
            self.df_structured_encoded = pd.get_dummies(self.clustercty[self.structured_cols], drop_first=True)
            
            # G·ªôp d·ªØ li·ªáu
            X_concat = pd.concat([
                self.df_structured_encoded.reset_index(drop=True), 
                df_tfidf.reset_index(drop=True)
            ], axis=1)
            
            # PCA theo code c·ªßa b·∫°n
            self.X_full = self.pca.fit_transform(X_concat)
            
            return True
            
        except Exception as e:
            st.error(f"L·ªói chu·∫©n b·ªã features: {e}")
            return False
    
    def find_optimal_clusters(self):
        """T√¨m s·ªë cluster t·ªëi ∆∞u theo code c·ªßa b·∫°n"""
        K = range(2, 11)
        silhouette_scores = []
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, k in enumerate(K):
            status_text.text(f'ƒêang test k={k}...')
            kmeans = KMeans(n_clusters=k, random_state=42)
            labels = kmeans.fit_predict(self.X_full)
            score = silhouette_score(self.X_full, labels)
            silhouette_scores.append(score)
            progress_bar.progress((i + 1) / len(K))
        
        status_text.empty()
        progress_bar.empty()
        
        best_k = K[silhouette_scores.index(max(silhouette_scores))]
        
        # Visualize silhouette scores
        fig = px.line(x=list(K), y=silhouette_scores, 
                     title="Silhouette Score vs Number of Clusters",
                     labels={'x': 'Number of Clusters (k)', 'y': 'Silhouette Score'})
        fig.add_vline(x=best_k, line_dash="dash", line_color="red", 
                     annotation_text=f"Best k={best_k}")
        st.plotly_chart(fig, use_container_width=True)
        
        return best_k, silhouette_scores
    
    def train_models(self):
        """Training c√°c m√¥ h√¨nh theo code c·ªßa b·∫°n"""
        try:
            # T√¨m s·ªë cluster t·ªëi ∆∞u
            best_k, silhouette_scores = self.find_optimal_clusters()
        
            # Clustering v·ªõi k t·ªëi ∆∞u
            final_kmeans = KMeans(n_clusters=best_k, random_state=42)
            cluster_labels = final_kmeans.fit_predict(self.X_full)
            self.clustercty['cluster'] = cluster_labels
        
            # Chia train/test
            X_train, X_test, y_train, y_test = train_test_split(
                self.X_full, cluster_labels, test_size=0.2, random_state=42
            )
        
            # C√°c m√¥ h√¨nh theo code c·ªßa b·∫°n
            models = {
                "Random Forest": RandomForestClassifier(),
                "Logistic Regression": LogisticRegression(max_iter=1000),
                "Naive Bayes": GaussianNB(),
                "SVM": SVC(),
                "Decision Tree": DecisionTreeClassifier(),
                "KNN": KNeighborsClassifier(),
                "Gradient Boosting": GradientBoostingClassifier()
            }
        
            results = []
            trained_models = {}  #  L∆∞u c√°c m√¥ h√¨nh ƒë√£ train
        
            progress_bar = st.progress(0)
            status_text = st.empty()
        
            for i, (name, model) in enumerate(models.items()):
                status_text.text(f'Training {name}...')
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                acc = accuracy_score(y_test, y_pred)
                results.append((name, acc))
                trained_models[name] = model  
                progress_bar.progress((i + 1) / len(models))
        
            status_text.empty()
            progress_bar.empty()
            
             #  Ch·ªçn v√† S·ª¨ D·ª§NG m√¥ h√¨nh t·ªët nh·∫•t
            best_model_name, best_acc = max(results, key=lambda x: x[1])
            self.best_model = trained_models[best_model_name]  #  S·ª≠ d·ª•ng m√¥ h√¨nh t·ªët nh·∫•t th·ª±c s·ª±
        
            # L∆∞u th√¥ng tin m√¥ h√¨nh t·ªët nh·∫•t
            st.success(f"üèÜ M√¥ h√¨nh t·ªët nh·∫•t: {best_model_name} v·ªõi accuracy: {best_acc:.3f}")
        
            return results, best_model_name, best_acc, best_k
        
        except Exception as e:
            st.error(f"L·ªói training models: {e}")
            return [], "", 0, 0
    
    def recommend_companies(self, user_input, text_input, threshold=0.1):
        """ƒê·ªÅ xu·∫•t c√¥ng ty theo code c·ªßa b·∫°n"""
        try:
            # X·ª≠ l√Ω text input theo code c·ªßa b·∫°n
            cleaned_text = self.text_processor.clean_pipeline(text_input)
            tfidf_vec = self.tfidf.transform([cleaned_text])
            
            # X·ª≠ l√Ω structured input theo code c·ªßa b·∫°n
            structured_df = pd.DataFrame([user_input])
            structured_encoded = pd.get_dummies(structured_df)
            
            # ƒê·∫£m b·∫£o c√≥ ƒë·ªß columns nh∆∞ trong code c·ªßa b·∫°n
            missing_cols = set(self.df_structured_encoded.columns) - set(structured_encoded.columns)
            for col in missing_cols:
                structured_encoded[col] = 0
            structured_encoded = structured_encoded[self.df_structured_encoded.columns]
            
            # G·ªôp features theo code c·ªßa b·∫°n
            user_input_vector = pd.concat([
                structured_encoded.reset_index(drop=True), 
                pd.DataFrame(tfidf_vec.toarray(), columns=self.tfidf.get_feature_names_out())
            ], axis=1)
            
            # PCA transform theo code c·ªßa b·∫°n
            user_input_pca = self.pca.transform(user_input_vector)
            
            # Predict cluster theo code c·ªßa b·∫°n
            predicted_cluster = self.best_model.predict(user_input_pca)[0]
            
            # T√≠nh Cosine Similarity theo code c·ªßa b·∫°n
            company_text_vectors = self.tfidf.transform(self.clustercty['combined_text'])
            similarity_scores = cosine_similarity(tfidf_vec, company_text_vectors).flatten()
            self.clustercty['similarity_score'] = similarity_scores
            
            # L·ªçc c√¥ng ty theo code c·ªßa b·∫°n
            matched = self.clustercty[
                (self.clustercty['cluster'] == predicted_cluster) & 
                (self.clustercty['similarity_score'] >= threshold)
            ].copy()
            
            matched = matched.sort_values(by='similarity_score', ascending=False).head(10)
            
            return matched, predicted_cluster
            
        except Exception as e:
            st.error(f"L·ªói ƒë·ªÅ xu·∫•t: {e}")
            return pd.DataFrame(), -1

class PySparkMLSystem:
    def __init__(self):
        self.spark = None
        self.spark_df_ml = None
        self.pyspark_results = {}
        
    def initialize_spark(self):
        """Kh·ªüi t·∫°o Spark Session v·ªõi error handling t·ªët h∆°n"""
        try:
            # Ki·ªÉm tra Java environment
            import os
            java_home = os.environ.get('JAVA_HOME')
            if not java_home:
                st.warning("‚ö†Ô∏è JAVA_HOME kh√¥ng ƒë∆∞·ª£c set. ƒêang th·ª≠ kh·ªüi t·∫°o Spark...")
        
            # C·∫•u h√¨nh Spark v·ªõi c√°c settings an to√†n h∆°n
            from pyspark.sql import SparkSession
        
            self.spark = SparkSession.builder \
                .appName("CompanyRecommendation") \
                .config("spark.driver.memory", "1g") \
                .config("spark.executor.memory", "1g") \
                .config("spark.sql.adaptive.enabled", "true") \
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
                .config("spark.driver.host", "localhost") \
                .getOrCreate()
        
            # Test Spark ho·∫°t ƒë·ªông
            test_df = self.spark.createDataFrame([(1, "test")], ["id", "value"])
            test_df.count()  # Test operation
        
            # Gi·∫£m log level
            self.spark.sparkContext.setLogLevel("ERROR")
        
            st.success(" Spark ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng!")
            return True
        
        except Exception as e:
            error_msg = str(e)
            if "JavaPackage" in error_msg:
                st.error("""
             **L·ªói Java/Spark Environment**
            
            **Nguy√™n nh√¢n c√≥ th·ªÉ:**
            - Java kh√¥ng ƒë∆∞·ª£c c√†i ƒë·∫∑t ho·∫∑c JAVA_HOME kh√¥ng ƒë√∫ng
            - PySpark kh√¥ng t∆∞∆°ng th√≠ch v·ªõi Java version
            - Spark kh√¥ng ƒë∆∞·ª£c c·∫•u h√¨nh ƒë√∫ng
            
            **Gi·∫£i ph√°p:**
            1. C√†i ƒë·∫∑t Java 8 ho·∫∑c 11: `sudo apt install openjdk-11-jdk`
            2. Set JAVA_HOME: `export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64`
            3. Restart terminal v√† th·ª≠ l·∫°i
            
            **Ho·∫∑c ch·ªâ s·ª≠ d·ª•ng Sklearn models ƒë·ªÉ ti·∫øp t·ª•c.**
            """)
            else:
                st.error(f" L·ªói kh·ªüi t·∫°o Spark: {error_msg}")
        
            return False
    
    def prepare_spark_data(self, clustercty, X_concat, cluster_labels):
        """Chu·∫©n b·ªã d·ªØ li·ªáu cho PySpark"""
        try:
            # Schema cho Spark DataFrame
            schema = StructType([
                StructField("id", IntegerType(), True),
                StructField("Company Name", StringType(), True),
                StructField("Company Type", StringType(), True),
                StructField("Company industry", StringType(), True),
                StructField("Company size", StringType(), True),
                StructField("Country", StringType(), True),
                StructField("Working days", StringType(), True),
                StructField("Overtime Policy", StringType(), True),
                StructField("Company overview_new", StringType(), True),
                StructField("Why you'll love working here_new", StringType(), True),
                StructField("Our key skills_new", StringType(), True),
                StructField("keyword", StringType(), True),
                StructField("sentiment_group", StringType(), True),
                StructField("combined_text", StringType(), True),
                StructField("cluster", IntegerType(), True)
            ])
            
            # Th√™m cluster labels v√†o X_concat
            X_concat_with_labels = X_concat.copy()
            X_concat_with_labels['cluster'] = cluster_labels
            
            # Convert to Spark DataFrame
            self.spark_df_ml = self.spark.createDataFrame(X_concat_with_labels)
            
            # Assemble features
            feature_columns = X_concat.columns.tolist()
            assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")
            self.spark_df_ml = assembler.transform(self.spark_df_ml)
            
            # Index labels
            indexer = StringIndexer(inputCol="cluster", outputCol="label")
            self.spark_df_ml = indexer.fit(self.spark_df_ml).transform(self.spark_df_ml)
            
            # Select features and labels
            self.spark_df_ml = self.spark_df_ml.select("features", "label")
            
            return True
            
        except Exception as e:
            st.error(f"L·ªói chu·∫©n b·ªã d·ªØ li·ªáu Spark: {e}")
            return False
    
    def train_pyspark_models(self):
        """Training PySpark models"""
        try:
            # Split data
            train_data, test_data = self.spark_df_ml.randomSplit([0.8, 0.2], seed=42)
            
            # Evaluator
            evaluator = MulticlassClassificationEvaluator(
                labelCol="label", 
                predictionCol="prediction", 
                metricName="accuracy"
            )
            
            # Logistic Regression
            lr = SparkLogisticRegression(featuresCol="features", labelCol="label")
            lr_model = lr.fit(train_data)
            lr_predictions = lr_model.transform(test_data)
            lr_accuracy = evaluator.evaluate(lr_predictions)
            
            # Decision Tree
            dt = SparkDecisionTreeClassifier(featuresCol="features", labelCol="label")
            dt_model = dt.fit(train_data)
            dt_predictions = dt_model.transform(test_data)
            dt_accuracy = evaluator.evaluate(dt_predictions)
            
            self.pyspark_results = {
                "PySpark Logistic Regression": lr_accuracy,
                "PySpark Decision Tree": dt_accuracy
            }
            
            return self.pyspark_results
            
        except Exception as e:
            st.error(f"L·ªói training PySpark models: {e}")
            return {}
    
    def stop_spark(self):
        """D·ª´ng Spark Session"""
        if self.spark:
            self.spark.stop()

# D·ªØ li·ªáu m·∫´u cho Company Suggest (gi·ªØ nguy√™n)
companies_data = {
    "Google": {
        "overview": "Google l√† m·ªôt trong nh·ªØng c√¥ng ty c√¥ng ngh·ªá h√†ng ƒë·∫ßu th·∫ø gi·ªõi, chuy√™n v·ªÅ t√¨m ki·∫øm tr·ª±c tuy·∫øn, ƒëi·ªán to√°n ƒë√°m m√¢y v√† c√¥ng ngh·ªá qu·∫£ng c√°o.",
        "industry": "Technology",
        "size": "Large (100,000+ employees)",
        "country": "USA",
        "similar": ["Microsoft", "Apple", "Amazon"]
    },
    "Microsoft": {
        "overview": "Microsoft l√† c√¥ng ty ph·∫ßn m·ªÅm ƒëa qu·ªëc gia, n·ªïi ti·∫øng v·ªõi h·ªá ƒëi·ªÅu h√†nh Windows v√† b·ªô ·ª©ng d·ª•ng Office.",
        "industry": "Technology", 
        "size": "Large (100,000+ employees)",
        "country": "USA",
        "similar": ["Google", "Apple", "IBM"]
    },
    "Apple": {
        "overview": "Apple l√† c√¥ng ty c√¥ng ngh·ªá thi·∫øt k·∫ø v√† s·∫£n xu·∫•t c√°c s·∫£n ph·∫©m ƒëi·ªán t·ª≠ ti√™u d√πng, ph·∫ßn m·ªÅm m√°y t√≠nh v√† d·ªãch v·ª• tr·ª±c tuy·∫øn.",
        "industry": "Technology",
        "size": "Large (100,000+ employees)", 
        "country": "USA",
        "similar": ["Google", "Microsoft", "Samsung"]
    },
    "Amazon": {
        "overview": "Amazon l√† c√¥ng ty th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ v√† ƒëi·ªán to√°n ƒë√°m m√¢y ƒëa qu·ªëc gia c√≥ tr·ª• s·ªü t·∫°i Seattle, Washington.",
        "industry": "E-commerce/Cloud",
        "size": "Large (1,000,000+ employees)",
        "country": "USA", 
        "similar": ["Google", "Microsoft", "Alibaba"]
    }
}

# Sidebar
st.sidebar.markdown("## ü§ñ AI Company Recommendation System")
st.sidebar.markdown("---")

# Menu categories
menu_options = ["Business Objective", "Company Suggest", "Build Project", "New Prediction"]
selected_menu = st.sidebar.selectbox("üìã Select Category:", menu_options)

# Th√¥ng tin ng∆∞·ªùi t·∫°o
st.sidebar.markdown("---")
st.sidebar.markdown("### üë• Creators Information")

# st.sidebar.markdown("**Creator 1:**")
# st.sidebar.markdown("üìß V√µ Minh Tr√≠")
# st.sidebar.markdown("‚úâÔ∏è trivm203@gmail.com")

st.sidebar.markdown("**Creator 2:**") 
st.sidebar.markdown("üìß Ph·∫°m Th·ªã Thu Th·∫£o")
st.sidebar.markdown("‚úâÔ∏è phamthithuthao@email.com")


# Main content
if selected_menu == "Business Objective":
    st.markdown('<h1 class="main-header">üéØ Business Objective</h1>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="section-header">M·ª•c ti√™u c·ªßa d·ª± √°n</div>
    
    H·ªá th·ªëng ƒë·ªÅ xu·∫•t c√¥ng ty n√†y ƒë∆∞·ª£c ph√°t tri·ªÉn nh·∫±m:
    
    ### üéØ M·ª•c ti√™u ch√≠nh:
    - **H·ªó tr·ª£ ·ª©ng vi√™n**: Gi√∫p ·ª©ng vi√™n t√¨m ki·∫øm c√¥ng ty ph√π h·ª£p v·ªõi mong mu·ªën v√† k·ªπ nƒÉng c·ªßa h·ªç
    - **T·ªëi ∆∞u h√≥a tuy·ªÉn d·ª•ng**: C·∫£i thi·ªán qu√° tr√¨nh matching gi·ªØa ·ª©ng vi√™n v√† nh√† tuy·ªÉn d·ª•ng
    - **Ph√¢n t√≠ch th·ªã tr∆∞·ªùng**: Cung c·∫•p insights v·ªÅ xu h∆∞·ªõng tuy·ªÉn d·ª•ng v√† y√™u c·∫ßu c√¥ng vi·ªác
    
    ### üîç T√≠nh nƒÉng ch√≠nh:
    1. **Ph√¢n t√≠ch c√¥ng ty**: Cung c·∫•p th√¥ng tin chi ti·∫øt v·ªÅ c√°c c√¥ng ty h√†ng ƒë·∫ßu
    2. **ƒê·ªÅ xu·∫•t th√¥ng minh**: S·ª≠ d·ª•ng machine learning ƒë·ªÉ ƒë·ªÅ xu·∫•t c√¥ng ty ph√π h·ª£p
    3. **So s√°nh c√¥ng ty**: Gi√∫p ·ª©ng vi√™n so s√°nh c√°c l·ª±a ch·ªçn kh√°c nhau
    4. **D·ª± ƒëo√°n xu h∆∞·ªõng**: Ph√¢n t√≠ch v√† d·ª± ƒëo√°n xu h∆∞·ªõng tuy·ªÉn d·ª•ng
    
    ### üìä L·ª£i √≠ch:
    - Ti·∫øt ki·ªám th·ªùi gian t√¨m ki·∫øm vi·ªác l√†m
    - TƒÉng t·ª∑ l·ªá match th√†nh c√¥ng
    - Cung c·∫•p th√¥ng tin minh b·∫°ch v·ªÅ th·ªã tr∆∞·ªùng lao ƒë·ªông
    - H·ªó tr·ª£ quy·∫øt ƒë·ªãnh ngh·ªÅ nghi·ªáp
    
    ### ü§ñ AI Features:
    - **Vietnamese Text Processing**: X·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát v·ªõi teencode, stopwords
    - **TF-IDF Vectorization**: Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh vector s·ªë
    - **K-means Clustering**: Ph√¢n nh√≥m c√¥ng ty theo ƒë·∫∑c ƒëi·ªÉm
    - **Multiple ML Models**: Random Forest, SVM, Logistic Regression, etc.
    - **Cosine Similarity**: T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng th√¥ng minh
    """, unsafe_allow_html=True)

elif selected_menu == "Company Suggest":
    st.markdown('<h1 class="main-header">üè¢ Company Suggestions</h1>', unsafe_allow_html=True)
    
    # Select box ch·ªçn c√¥ng ty
    selected_company = st.selectbox("üîç Ch·ªçn c√¥ng ty ƒë·ªÉ xem th√¥ng tin:", list(companies_data.keys()))
    
    if selected_company:
        company_info = companies_data[selected_company]
        
        # Hi·ªÉn th·ªã th√¥ng tin c√¥ng ty ƒë∆∞·ª£c ch·ªçn
        st.markdown(f'<div class="section-header">üìã Th√¥ng tin v·ªÅ {selected_company}</div>', unsafe_allow_html=True)
        
        st.markdown(f"""
        <div class="company-card">
            <h4>üè¢ {selected_company}</h4>
            <p><strong>T·ªïng quan:</strong> {company_info['overview']}</p>
            <p><strong>Ng√†nh:</strong> {company_info['industry']}</p>
            <p><strong>Quy m√¥:</strong> {company_info['size']}</p>
            <p><strong>Qu·ªëc gia:</strong> {company_info['country']}</p>
        </div>
        """, unsafe_allow_html=True)
        
        # Hi·ªÉn th·ªã c√°c c√¥ng ty t∆∞∆°ng t·ª±
        st.markdown('<div class="section-header">üîó C√°c c√¥ng ty t∆∞∆°ng t·ª±</div>', unsafe_allow_html=True)
        
        cols = st.columns(len(company_info['similar']))
        
        for idx, similar_company in enumerate(company_info['similar']):
            with cols[idx]:
                st.markdown(f"**{similar_company}**")
                
                # Expander cho th√¥ng tin chi ti·∫øt
                with st.expander(f"Xem th√¥ng tin {similar_company}"):
                    if similar_company in companies_data:
                        similar_info = companies_data[similar_company]
                        st.write(f"**T·ªïng quan:** {similar_info['overview']}")
                        st.write(f"**Ng√†nh:** {similar_info['industry']}")
                        st.write(f"**Quy m√¥:** {similar_info['size']}")
                        st.write(f"**Qu·ªëc gia:** {similar_info['country']}")

elif selected_menu == "Build Project":
    st.markdown('<h1 class="main-header">üî® Build Project</h1>', unsafe_allow_html=True)
    
    # Kh·ªüi t·∫°o v√† load d·ªØ li·ªáu
    if not st.session_state.data_loaded:
        with st.spinner("üîÑ ƒêang load d·ªØ li·ªáu..."):
            ml_system = CompanyRecommendationSystem()
            if ml_system.load_data():
                st.session_state.ml_system = ml_system
                st.session_state.data_loaded = True
                st.success("‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c load th√†nh c√¥ng!")
            else:
                st.error("‚ùå Kh√¥ng th·ªÉ load d·ªØ li·ªáu. Vui l√≤ng ki·ªÉm tra file d·ªØ li·ªáu.")
                st.stop()
    
    # T√πy ch·ªçn training
    st.markdown("### üöÄ Ch·ªçn ph∆∞∆°ng th·ª©c Training:")
    
    col1, col2 = st.columns(2)
    
    with col1:
        sklearn_training = st.button("üî¨ Train Sklearn Models", use_container_width=True)
    
    with col2:
        if PYSPARK_AVAILABLE:
            pyspark_training = st.button("‚ö° Train PySpark Models", use_container_width=True)
        else:
            st.button("‚ö° PySpark Not Available", disabled=True, use_container_width=True)
    
    # Sklearn Training
    if sklearn_training and st.session_state.data_loaded:
        if not st.session_state.model_trained:
            with st.spinner("ü§ñ ƒêang chu·∫©n b·ªã features v√† training sklearn models..."):
                ml_system = st.session_state.ml_system
                
                if ml_system.prepare_features():
                    results, best_model_name, best_acc, best_k = ml_system.train_models()
                    
                    if results:
                        st.session_state.model_trained = True
                        st.session_state.training_results = results
                        st.session_state.best_model_name = best_model_name
                        st.session_state.best_acc = best_acc
                        st.session_state.best_k = best_k
                        st.success("‚úÖ Sklearn Training ho√†n t·∫•t!")
                        st.rerun()
    
   # PySpark Training
    if PYSPARK_AVAILABLE and 'pyspark_training' in locals() and pyspark_training:
        if not st.session_state.get('pyspark_trained', False):
            with st.spinner("‚ö° ƒêang kh·ªüi t·∫°o Spark v√† training PySpark models..."):
                # C·∫ßn sklearn models tr∆∞·ªõc
                if not st.session_state.model_trained:
                    st.warning("‚ö†Ô∏è Vui l√≤ng train sklearn models tr∆∞·ªõc!")
                else:
                    try:
                        ml_system = st.session_state.ml_system
                        pyspark_system = PySparkMLSystem()
                    
                        if pyspark_system.initialize_spark():
                            # Chu·∫©n b·ªã d·ªØ li·ªáu t·ª´ sklearn pipeline
                            X_concat = pd.concat([
                                ml_system.df_structured_encoded.reset_index(drop=True),
                                pd.DataFrame(
                                    ml_system.tfidf.transform(ml_system.clustercty['combined_text']).toarray(),
                                    columns=ml_system.tfidf.get_feature_names_out()
                                )
                            ], axis=1)
                        
                            cluster_labels = ml_system.clustercty['cluster'].values
                        
                            if pyspark_system.prepare_spark_data(ml_system.clustercty, X_concat, cluster_labels):
                                pyspark_results = pyspark_system.train_pyspark_models()
                            
                                if pyspark_results:
                                    st.session_state.pyspark_trained = True
                                    st.session_state.pyspark_results = pyspark_results
                                    st.session_state.pyspark_system = pyspark_system
                                    st.success("‚úÖ PySpark Training ho√†n t·∫•t!")
                                    st.rerun()
                        
                            pyspark_system.stop_spark()
                        else:
                            st.info("üí° **Kh√¥ng th·ªÉ kh·ªüi t·∫°o PySpark.** B·∫°n v·∫´n c√≥ th·ªÉ s·ª≠ d·ª•ng Sklearn models ƒë·ªÉ ti·∫øp t·ª•c d·ª± √°n!")
                            
                    except Exception as e:
                        st.error(f"‚ùå L·ªói trong qu√° tr√¨nh PySpark training: {e}")
                        st.info("üí° H√£y s·ª≠ d·ª•ng Sklearn models ƒë·ªÉ ti·∫øp t·ª•c!")
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£ so s√°nh
    if st.session_state.get('model_trained', False):
        st.markdown("""
        <div class="section-header">üìä K·∫øt qu·∫£ nghi√™n c·ª©u v√† so s√°nh m√¥ h√¨nh</div>
        
        ### üß™ Ph∆∞∆°ng ph√°p nghi√™n c·ª©u:
        
        #### 1. Thu th·∫≠p d·ªØ li·ªáu
        - **Ngu·ªìn d·ªØ li·ªáu**: IT_Viec
        
        #### 2. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
        - L√†m s·∫°ch d·ªØ li·ªáu thi·∫øu v√† b·∫•t th∆∞·ªùng
        - X·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát (teencode, stopwords, wrong words)
        - TF-IDF Vectorization v·ªõi 500 features
        - One-hot encoding cho d·ªØ li·ªáu c√≥ c·∫•u tr√∫c
        - PCA gi·∫£m chi·ªÅu xu·ªëng 50 components
        
        #### 3. So s√°nh Sklearn vs PySpark
        - **Sklearn**: Ph√π h·ª£p cho datasets nh·ªè-trung b√¨nh, d·ªÖ s·ª≠ d·ª•ng
        - **PySpark**: Scalable cho big data, distributed computing
        - **Performance**: So s√°nh accuracy v√† th·ªùi gian training
        """, unsafe_allow_html=True)
        
        # Metrics overview
        ml_system = st.session_state.ml_system
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("üìä S·ªë c√¥ng ty", len(ml_system.clustercty))
        with col2:
            st.metric("üéØ S·ªë clusters", st.session_state.best_k)
        with col3:
            st.metric("üèÜ Best Sklearn", st.session_state.best_model_name)
        with col4:
            st.metric("üìà Best Accuracy", f"{st.session_state.best_acc:.3f}")
        
        # Sklearn Results
        st.markdown("### üî¨ Sklearn Models Results:")
        sklearn_results_df = pd.DataFrame(st.session_state.training_results, columns=['M√¥ h√¨nh', 'Accuracy'])
        sklearn_results_df['Accuracy (%)'] = (sklearn_results_df['Accuracy'] * 100).round(2)
        sklearn_results_df['Framework'] = 'Sklearn'
        
        st.dataframe(sklearn_results_df[['M√¥ h√¨nh', 'Accuracy (%)', 'Framework']], use_container_width=True)
        
        # PySpark Results (if available)
        if st.session_state.get('pyspark_trained', False):
            st.markdown("### ‚ö° PySpark Models Results:")
            pyspark_results = st.session_state.pyspark_results
            pyspark_results_df = pd.DataFrame(list(pyspark_results.items()), columns=['M√¥ h√¨nh', 'Accuracy'])
            pyspark_results_df['Accuracy (%)'] = (pyspark_results_df['Accuracy'] * 100).round(2)
            pyspark_results_df['Framework'] = 'PySpark'
            
            st.dataframe(pyspark_results_df[['M√¥ h√¨nh', 'Accuracy (%)', 'Framework']], use_container_width=True)
            
            # Combined comparison
            st.markdown("### üÜö Sklearn vs PySpark Comparison:")
            
            # Combine results for comparison
            combined_results = pd.concat([
                sklearn_results_df[['M√¥ h√¨nh', 'Accuracy (%)', 'Framework']],
                pyspark_results_df[['M√¥ h√¨nh', 'Accuracy (%)', 'Framework']]
            ], ignore_index=True)
            
            # Comparison chart
            fig_comparison = px.bar(
                combined_results, 
                x='M√¥ h√¨nh', 
                y='Accuracy (%)',
                color='Framework',
                title="Sklearn vs PySpark Models Comparison",
                barmode='group'
            )
            fig_comparison.update_layout(xaxis_tickangle=-45)
            st.plotly_chart(fig_comparison, use_container_width=True)
            
            # Performance analysis
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### üî¨ Sklearn Advantages:")
                st.markdown("""
                - ‚úÖ D·ªÖ s·ª≠ d·ª•ng v√† debug
                - ‚úÖ Rich ecosystem v√† documentation
                - ‚úÖ Ph√π h·ª£p cho prototyping
                - ‚úÖ Nhi·ªÅu algorithms c√≥ s·∫µn
                - ‚úÖ T√≠ch h·ª£p t·ªët v·ªõi pandas/numpy
                """)
            
            with col2:
                st.markdown("#### ‚ö° PySpark Advantages:")
                st.markdown("""
                - ‚úÖ Scalable cho big data
                - ‚úÖ Distributed computing
                - ‚úÖ Memory optimization
                - ‚úÖ Fault tolerance
                - ‚úÖ Integration v·ªõi Hadoop ecosystem
                """)
            
            # Best model comparison
            best_sklearn = sklearn_results_df.loc[sklearn_results_df['Accuracy (%)'].idxmax()]
            best_pyspark = pyspark_results_df.loc[pyspark_results_df['Accuracy (%)'].idxmax()]
            
            st.markdown("### üèÜ Best Models Comparison:")
            
            comparison_metrics = pd.DataFrame({
                'Metric': ['Best Model', 'Accuracy (%)', 'Framework'],
                'Sklearn': [best_sklearn['M√¥ h√¨nh'], best_sklearn['Accuracy (%)'], 'Sklearn'],
                'PySpark': [best_pyspark['M√¥ h√¨nh'], best_pyspark['Accuracy (%)'], 'PySpark']
            })
            
            st.dataframe(comparison_metrics, use_container_width=True)
            
        else:
            st.info("üí° Train PySpark models ƒë·ªÉ xem so s√°nh chi ti·∫øt!")
        
        # Sklearn visualization
        fig_sklearn = px.bar(
            sklearn_results_df, 
            x='M√¥ h√¨nh', 
            y='Accuracy (%)', 
            title="Sklearn Models Performance",
            color='Accuracy (%)',
            color_continuous_scale='viridis'
        )
        fig_sklearn.update_layout(xaxis_tickangle=-45)
        st.plotly_chart(fig_sklearn, use_container_width=True)
        
        # Cluster analysis
        st.markdown("### üéØ Ph√¢n t√≠ch Clusters:")
        cluster_analysis = ml_system.clustercty.groupby('cluster').agg({
            'Company Name': 'count',
            'Company industry': lambda x: x.mode().iloc[0] if not x.mode().empty else 'N/A',
            'Company size': lambda x: x.mode().iloc[0] if not x.mode().empty else 'N/A',
            'sentiment_group': lambda x: x.mode().iloc[0] if not x.mode().empty else 'N/A'
        }).round(2)
        
        cluster_analysis.columns = ['S·ªë c√¥ng ty', 'Ng√†nh ch·ªß ƒë·∫°o', 'Quy m√¥ ch·ªß ƒë·∫°o', 'Sentiment ch·ªß ƒë·∫°o']
        st.dataframe(cluster_analysis, use_container_width=True)
        
        # Final conclusions
        conclusion_text = f"""
        ### üèÜ K·∫øt lu·∫≠n:
        - **Sklearn Best**: {st.session_state.best_model_name} v·ªõi accuracy {st.session_state.best_acc:.3f}
        - **S·ªë clusters t·ªëi ∆∞u**: {st.session_state.best_k} clusters
        - **T·ªïng s·ªë c√¥ng ty**: {len(ml_system.clustercty)} c√¥ng ty
        - **Features**: {ml_system.X_full.shape[1]} features sau PCA
        """
        
        if st.session_state.get('pyspark_trained', False):
            pyspark_best = max(st.session_state.pyspark_results.items(), key=lambda x: x[1])
            conclusion_text += f"""
        - **PySpark Best**: {pyspark_best[0]} v·ªõi accuracy {pyspark_best[1]:.3f}
        - **Framework Winner**: {'Sklearn' if st.session_state.best_acc > pyspark_best[1] else 'PySpark'}
            """
        
        conclusion_text += """
        
        ### üîç Insights quan tr·ªçng:
        1. **Text processing** ƒë√≥ng vai tr√≤ quan tr·ªçng trong vi·ªác ph√¢n lo·∫°i c√¥ng ty
        2. **Clustering** gi√∫p nh√≥m c√°c c√¥ng ty c√≥ ƒë·∫∑c ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng
        3. **Sentiment analysis** cung c·∫•p th√¥ng tin v·ªÅ vƒÉn h√≥a c√¥ng ty
        4. **Framework choice** ph·ª• thu·ªôc v√†o k√≠ch th∆∞·ªõc d·ªØ li·ªáu v√† y√™u c·∫ßu scalability
        5. **Sklearn** ph√π h·ª£p cho datasets nh·ªè-trung b√¨nh v·ªõi ƒë·ªô ch√≠nh x√°c cao
        6. **PySpark** c·∫ßn thi·∫øt khi scale l√™n big data v√† distributed computing
        """
        
        st.markdown(conclusion_text, unsafe_allow_html=True)
    
    else:
        st.info("üëÜ Nh·∫•n n√∫t 'Train Sklearn Models' ƒë·ªÉ b·∫Øt ƒë·∫ßu ph√¢n t√≠ch!")

elif selected_menu == "New Prediction":
    st.markdown('<h1 class="main-header">ü§ñ AI-Powered Company Recommendation</h1>', unsafe_allow_html=True)
    
    # Ki·ªÉm tra d·ªØ li·ªáu v√† model ƒë√£ s·∫µn s√†ng
    if not st.session_state.data_loaded or not st.session_state.model_trained:
        st.warning("‚ö†Ô∏è Vui l√≤ng v√†o m·ª•c 'Build Project' ƒë·ªÉ load d·ªØ li·ªáu v√† train model tr∆∞·ªõc!")
        st.stop()
    
    ml_system = st.session_state.ml_system
    
    st.success("‚úÖ H·ªá th·ªëng AI ƒë√£ s·∫µn s√†ng v·ªõi d·ªØ li·ªáu th·ª±c!")
    
    # Form nh·∫≠p li·ªáu
    st.markdown('<div class="prediction-container">', unsafe_allow_html=True)
    st.markdown("### ü§ñ Nh·∫≠p th√¥ng tin ƒë·ªÉ nh·∫≠n ƒë·ªÅ xu·∫•t t·ª´ AI")
    
    with st.form("ai_prediction_form"):
        col1, col2 = st.columns(2)
        
        # L·∫•y options t·ª´ d·ªØ li·ªáu th·ª±c
        with col1:
            company_type_options = sorted(ml_system.clustercty['Company Type'].dropna().unique().tolist())
            company_type = st.selectbox("üè¢ Company Type:", company_type_options)
            
            industry_options = sorted(ml_system.clustercty['Company industry'].dropna().unique().tolist())
            company_industry = st.selectbox("üè≠ Company Industry:", industry_options)
            
            size_options = sorted(ml_system.clustercty['Company size'].dropna().unique().tolist())
            company_size = st.selectbox("üë• Company Size:", size_options)
        
        with col2:
            country_options = sorted(ml_system.clustercty['Country'].dropna().unique().tolist())
            country = st.selectbox("üåç Country:", country_options)
            
            working_days_options = sorted(ml_system.clustercty['Working days'].dropna().unique().tolist())
            working_days = st.selectbox("üìÖ Working Days:", working_days_options)
            
            overtime_options = sorted(ml_system.clustercty['Overtime Policy'].dropna().unique().tolist())
            overtime_policy = st.selectbox("‚è∞ Overtime Policy:", overtime_options)
        
        # Text input cho mong mu·ªën
        st.markdown("### üí≠ M√¥ t·∫£ mong mu·ªën c·ªßa b·∫°n:")
        user_expectations = st.text_area(
            "H√£y chia s·∫ª chi ti·∫øt v·ªÅ c√¥ng ty v√† c√¥ng vi·ªác l√Ω t∆∞·ªüng:",
            placeholder="V√≠ d·ª•: T√¥i mu·ªën l√†m vi·ªác trong m√¥i tr∆∞·ªùng c√¥ng ngh·ªá nƒÉng ƒë·ªông, c√≥ c∆° h·ªôi h·ªçc AI/ML, l∆∞∆°ng cao, work-life balance t·ªët, ƒë·ªôi ng≈© tr·∫ª trung s√°ng t·∫°o...",
            height=120
        )
        
        # Threshold slider
        threshold = st.slider(
            "üéØ ƒê·ªô t∆∞∆°ng ƒë·ªìng t·ªëi thi·ªÉu:",
            min_value=0.0,
            max_value=0.5,
            value=0.1,
            step=0.01,
            help="ƒêi·ªÅu ch·ªânh ƒë·ªÉ l·ªçc c√°c c√¥ng ty ph√π h·ª£p h∆°n"
        )
        
        submitted = st.form_submit_button("üöÄ T√¨m ki·∫øm v·ªõi AI", use_container_width=True)
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    # X·ª≠ l√Ω khi form ƒë∆∞·ª£c submit
    if submitted:
        if user_expectations.strip():
            with st.spinner("ü§ñ AI ƒëang ph√¢n t√≠ch v√† t√¨m ki·∫øm..."):
                user_input = {
                    'Company Type': company_type,
                    'Company industry': company_industry,
                    'Company size': company_size,
                    'Country': country,
                    'Working days': working_days,
                    'Overtime Policy': overtime_policy
                }
                
                recommendations, predicted_cluster = ml_system.recommend_companies(
                    user_input, user_expectations, threshold
                )
            
            if not recommendations.empty:
                st.markdown("## üéØ K·∫øt qu·∫£ t·ª´ AI")
                
                # Metrics
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("üéØ AI Cluster", f"Cluster {predicted_cluster}")
                with col2:
                    st.metric("üìä C√¥ng ty ph√π h·ª£p", len(recommendations))
                with col3:
                    avg_similarity = recommendations['similarity_score'].mean()
                    st.metric("üìà ƒê·ªô t∆∞∆°ng ƒë·ªìng TB", f"{avg_similarity:.3f}")
                with col4:
                    max_similarity = recommendations['similarity_score'].max()
                    st.metric("üèÜ ƒêi·ªÉm cao nh·∫•t", f"{max_similarity:.3f}")
                
                # Visualization
                if len(recommendations) > 1:
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        fig_hist = px.histogram(
                            recommendations, 
                            x='similarity_score',
                            title="üìä Ph√¢n b·ªë ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng AI",
                            labels={'similarity_score': 'ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng', 'count': 'S·ªë l∆∞·ª£ng'},
                            color_discrete_sequence=['#1f77b4']
                        )
                        st.plotly_chart(fig_hist, use_container_width=True)
                    
                    with col2:
                        fig_scatter = px.scatter(
                            recommendations.head(10), 
                            x=range(len(recommendations.head(10))),
                            y='similarity_score',
                            hover_data=['Company Name'],
                            title="üìà ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng theo th·ª© h·∫°ng",
                            labels={'x': 'Th·ª© h·∫°ng', 'similarity_score': 'ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng'}
                        )
                        st.plotly_chart(fig_scatter, use_container_width=True)
                
                # Detailed recommendations
                st.markdown("### üèÜ Top c√¥ng ty ƒë∆∞·ª£c AI ƒë·ªÅ xu·∫•t:")
                
                for idx, (_, company) in enumerate(recommendations.head(8).iterrows()):
                    with st.expander(f"üè¢ {company['Company Name']} - AI Score: {company['similarity_score']:.3f}"):
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.markdown(f"**üè≠ Ng√†nh:** {company.get('Company industry', 'N/A')}")
                            st.markdown(f"**üë• Quy m√¥:** {company.get('Company size', 'N/A')}")
                            st.markdown(f"**üåç Qu·ªëc gia:** {company.get('Country', 'N/A')}")
                            st.markdown(f"**üè¢ Lo·∫°i:** {company.get('Company Type', 'N/A')}")
                        
                        with col2:
                            st.markdown(f"**üìÖ L√†m vi·ªác:** {company.get('Working days', 'N/A')}")
                            st.markdown(f"**‚è∞ OT Policy:** {company.get('Overtime Policy', 'N/A')}")
                            st.markdown(f"**üòä Sentiment:** {company.get('sentiment_group', 'N/A')}")
                            st.markdown(f"**üîë Keywords:** {company.get('keyword', 'N/A')}")
                        
                        # Company description t·ª´ d·ªØ li·ªáu th·ª±c
                        if 'Company overview_new' in company:
                            st.markdown("**üìù M√¥ t·∫£ c√¥ng ty:**")
                            st.write(company.get('Company overview_new', 'Kh√¥ng c√≥ th√¥ng tin'))
                        
                        if "Why you'll love working here_new" in company:
                            st.markdown("**üíù T·∫°i sao b·∫°n s·∫Ω y√™u th√≠ch l√†m vi·ªác ·ªü ƒë√¢y:**")
                            st.write(company.get("Why you'll love working here_new", 'Kh√¥ng c√≥ th√¥ng tin'))
                        
                        # AI Score visualization
                        score = company['similarity_score']
                        if score > 0.3:
                            color, level = "green", "R·∫•t ph√π h·ª£p"
                        elif score > 0.15:
                            color, level = "orange", "Ph√π h·ª£p"
                        else:
                            color, level = "blue", "C√≥ th·ªÉ ph√π h·ª£p"
                            
                        st.markdown(f"""
                        <div style="background: linear-gradient(90deg, {color}20, {color}40); 
                                    padding: 0.8rem; border-radius: 8px; border-left: 4px solid {color};">
                            ü§ñ AI Confidence: {score:.3f} ({score*100:.1f}%) - <strong>{level}</strong>
                        </div>
                        """, unsafe_allow_html=True)
                        
                        # Action buttons
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            if st.button(f"üìß Li√™n h·ªá", key=f"ai_contact_{idx}"):
                                st.success("‚úÖ ƒê√£ l∆∞u th√¥ng tin li√™n h·ªá!")
                        with col2:
                            if st.button(f"üíæ L∆∞u", key=f"ai_save_{idx}"):
                                st.success("‚úÖ ƒê√£ th√™m v√†o danh s√°ch y√™u th√≠ch!")
                        with col3:
                            if st.button(f"üìä Ph√¢n t√≠ch", key=f"ai_analyze_{idx}"):
                                st.info("üîç T√≠nh nƒÉng ph√¢n t√≠ch chi ti·∫øt ƒëang ph√°t tri·ªÉn!")
                
                # Summary table
                st.markdown("### üìä B·∫£ng t·ªïng h·ª£p AI:")
                display_columns = ['Company Name', 'Company industry', 'Company size', 'Country', 'similarity_score', 'sentiment_group']
                available_columns = [col for col in display_columns if col in recommendations.columns]
                
                display_df = recommendations[available_columns].copy()
                display_df['similarity_score'] = display_df['similarity_score'].round(3)
                display_df = display_df.rename(columns={
                    'Company Name': 'T√™n c√¥ng ty',
                    'Company industry': 'Ng√†nh',
                    'Company size': 'Quy m√¥',
                    'Country': 'Qu·ªëc gia',
                    'similarity_score': 'ƒêi·ªÉm AI',
                    'sentiment_group': 'Sentiment'
                })
                
                st.dataframe(
                    display_df.style.background_gradient(subset=['ƒêi·ªÉm AI']),
                    use_container_width=True
                )
                
                # Download options
                col1, col2 = st.columns(2)
                with col1:
                    csv = display_df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="üì• T·∫£i xu·ªëng CSV",
                        data=csv,
                        file_name=f"ai_recommendations_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                with col2:
                    json_data = display_df.to_json(orient='records', force_ascii=False)
                    st.download_button(
                        label="üì• T·∫£i xu·ªëng JSON",
                        data=json_data,
                        file_name=f"ai_recommendations_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json"
                    )
                
            else:
                st.markdown("""
                <div class="warning-box">
                    ‚ö†Ô∏è <strong>AI kh√¥ng t√¨m th·∫•y c√¥ng ty ph√π h·ª£p!</strong><br>
                    H√£y th·ª≠:
                    <ul>
                        <li>Gi·∫£m ƒë·ªô t∆∞∆°ng ƒë·ªìng t·ªëi thi·ªÉu</li>
                        <li>M·ªü r·ªông ho·∫∑c thay ƒë·ªïi m√¥ t·∫£ mong mu·ªën</li>
                        <li>Th·ª≠ c√°c ti√™u ch√≠ l·ª±a ch·ªçn kh√°c</li>
                        <li>S·ª≠ d·ª•ng t·ª´ kh√≥a ƒë∆°n gi·∫£n h∆°n</li>
                    </ul>
                </div>
                """, unsafe_allow_html=True)
        else:
            st.error("‚ùå Vui l√≤ng nh·∫≠p m√¥ t·∫£ mong mu·ªën ƒë·ªÉ AI c√≥ th·ªÉ ph√¢n t√≠ch!")

# Footer
st.markdown("---")
st.markdown(
    "<div style='text-align: center; color: #666; padding: 1rem;'>"
    "Trung t√¢m Tin H·ªçc - Tr∆∞·ªùng ƒê·∫°i H·ªçc Khoa H·ªçc T·ª± Nhi√™n | Tri Vo and Thao Pham <br>"
    "</div>", 
    unsafe_allow_html=True
)
