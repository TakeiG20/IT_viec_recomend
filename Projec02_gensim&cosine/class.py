import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import re
import string
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, silhouette_score
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
import pickle
from gensim import corpora, models, similarities

try:
    from pyspark.sql import SparkSession
    from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType
    from pyspark.ml.feature import VectorAssembler, StringIndexer
    from pyspark.ml.classification import LogisticRegression as SparkLogisticRegression, DecisionTreeClassifier as SparkDecisionTreeClassifier
    from pyspark.ml.evaluation import MulticlassClassificationEvaluator
    from pyspark.ml import Pipeline
    PYSPARK_AVAILABLE = True
except ImportError:
    PYSPARK_AVAILABLE = False

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="AI Company Recommendation System AT IT_Viec",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS t√πy ch·ªânh
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .section-header {
        font-size: 1.8rem;
        font-weight: bold;
        color: #2c3e50;
        margin-bottom: 1rem;
    }
    .company-card {
        background-color: #f8f9fa;
        padding: 1rem;
        border-radius: 10px;
        border-left: 4px solid #1f77b4;
        margin-bottom: 1rem;
    }
    .cluster-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem;
        border-radius: 15px;
        color: white;
        margin-bottom: 1rem;
    }
    .sentiment-positive {
        background-color: #d4edda;
        border-left: 4px solid #28a745;
        padding: 1rem;
        border-radius: 8px;
        margin-bottom: 1rem;
    }
    .sentiment-negative {
        background-color: #f8d7da;
        border-left: 4px solid #dc3545;
        padding: 1rem;
        border-radius: 8px;
        margin-bottom: 1rem;
    }
    .sentiment-neutral {
        background-color: #fff3cd;
        border-left: 4px solid #ffc107;
        padding: 1rem;
        border-radius: 8px;
        margin-bottom: 1rem;
    }
    .prediction-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 15px;
        color: white;
        margin-bottom: 2rem;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        color: #856404;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
    .error-box {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        color: #721c24;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# Kh·ªüi t·∫°o session state
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
if 'model_trained' not in st.session_state:
    st.session_state.model_trained = False

# Class x·ª≠ l√Ω vƒÉn b·∫£n t√≠ch h·ª£p t·ª´ code c·ªßa b·∫°n
class TextProcessor:
    def __init__(self):
        self.teen_dict = {}
        self.stopwords_lst = []
        self.wrong_lst = []
        self.load_dictionaries()
    
    def load_dictionaries(self):
        """Load c√°c t·ª´ ƒëi·ªÉn x·ª≠ l√Ω vƒÉn b·∫£n t·ª´ files ho·∫∑c fallback"""
        try:
            # Load teencode t·ª´ file ho·∫∑c fallback
            teencode_paths = ['files/teencode.txt', 'teencode.txt']
            for path in teencode_paths:
                if os.path.exists(path):
                    with open(path, 'r', encoding="utf8") as file:
                        teen_lst = file.read().split('\n')
                        for line in teen_lst:
                            if '\t' in line:
                                key, value = line.split('\t', 1)
                                self.teen_dict[key] = str(value)
                    break
            else:
                # Fallback teencode dictionary
                self.teen_dict = {
                    'ko': 'kh√¥ng', 'k': 'kh√¥ng', 'dc': 'ƒë∆∞·ª£c', 'vs': 'v·ªõi',
                    'tks': 'thanks', 'ty': 'thank you', 'ok': 'okay', 'oke': 'okay',
                    'bt': 'b√¨nh th∆∞·ªùng', 'nc': 'n√≥i chuy·ªán', 'kb': 'k·∫øt b·∫°n'
                }
            
            # Load stopwords t·ª´ file ho·∫∑c fallback
            stopwords_paths = ['files/vietnamese-stopwords_rev.txt', 'vietnamese-stopwords_rev.txt']
            for path in stopwords_paths:
                if os.path.exists(path):
                    with open(path, 'r', encoding="utf8") as file:
                        self.stopwords_lst = file.read().split('\n')
                    break
            else:
                # Fallback stopwords
                self.stopwords_lst = [
                    'v√†', 'c·ªßa', 'c√≥', 'l√†', 'ƒë∆∞·ª£c', 'trong', 'v·ªõi', 'ƒë·ªÉ', 'cho', 't·ª´',
                    'm·ªôt', 'c√°c', 'n√†y', 'ƒë√≥', 'nh·ªØng', 'nhi·ªÅu', 'r·∫•t', 'c≈©ng', 's·∫Ω',
                    'ƒë√£', 'ƒëang', 'v·ªÅ', 'theo', 'nh∆∞', 'khi', 'n·∫øu', 'v√¨', 'do', 'b·ªüi'
                ]
            
            # Load wrong words t·ª´ file ho·∫∑c fallback
            wrong_words_paths = ['files/wrong-word_rev.txt', 'wrong-word_rev.txt']
            for path in wrong_words_paths:
                if os.path.exists(path):
                    with open(path, 'r', encoding="utf8") as file:
                        self.wrong_lst = file.read().split('\n')
                    break
            else:
                self.wrong_lst = ['zzz', 'xxx', 'aaa', 'bbb', 'ccc']
                    
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load m·ªôt s·ªë file t·ª´ ƒëi·ªÉn: {e}")
    
    def clean_text(self, text):
        """L√†m s·∫°ch vƒÉn b·∫£n c∆° b·∫£n - t√≠ch h·ª£p t·ª´ code c·ªßa b·∫°n"""
        if not text or pd.isna(text):
            return ""
        text = str(text).lower()  # Hoa -> th∆∞·ªùng
        text = re.sub(rf"[{string.punctuation}]", "", text)  # B·ªè d·∫•u c√¢u
        text = re.sub(r"\b(g|ml)\b", "", text)  # B·ªè t·ª´ 'g' ho·∫∑c 'ml' khi n√≥ l√† c·∫£ t·ª´
        text = re.sub(r"\s+", " ", text).strip()  # X√≥a kho·∫£ng tr·∫Øng th·ª´a
        text = re.sub(r"^[\-\+\*\‚Ä¢\‚óè\¬∑\~\‚Äì\‚Äî\>]+", "", text)  # B·ªè d·∫•u ƒë·∫ßu c√¢u
        return text
    
    def fix_teencode(self, text):
        """S·ª≠a teencode - t√≠ch h·ª£p t·ª´ code c·ªßa b·∫°n"""
        words = text.split()
        corrected = [self.teen_dict.get(word, word) for word in words]
        return " ".join(corrected)
    
    def remove_wrongword(self, text):
        """Lo·∫°i b·ªè t·ª´ sai"""
        words = text.split()
        trueword = [word for word in words if word not in self.wrong_lst]
        return " ".join(trueword)
    
    def remove_stopword(self, text):
        """Lo·∫°i b·ªè stopwords - t√≠ch h·ª£p t·ª´ code c·ªßa b·∫°n"""
        words = text.split()
        stopword = [word for word in words if word not in self.stopwords_lst]
        return " ".join(stopword)
    
    def clean_pipeline(self, text):
        """Pipeline x·ª≠ l√Ω vƒÉn b·∫£n ho√†n ch·ªânh - t√≠ch h·ª£p t·ª´ code c·ªßa b·∫°n"""
        if not text or pd.isna(text):
            return ""
        text = self.clean_text(text)
        text = self.fix_teencode(text)
        text = self.remove_wrongword(text)
        text = self.remove_stopword(text)
        return text

# Class h·ªá th·ªëng ML v·ªõi d·ªØ li·ªáu th·ª±c
class CompanyRecommendationSystem:
    def __init__(self):
        self.clustercty = None
        self.tfidf = TfidfVectorizer(max_features=500)
        self.pca = PCA(n_components=50, random_state=42)
        self.best_model = None
        self.df_structured_encoded = None
        self.X_full = None
        self.text_processor = TextProcessor()
        self.structured_cols = ['Company Type', 'Company industry', 'Company size', 'Country', 'Working days', 'Overtime Policy']
        self.text_cols = ['Company overview_new', "Why you'll love working here_new", 'Our key skills_new', 'keyword']
        
    @st.cache_data
    def load_data(_self):
        """Load v√† x·ª≠ l√Ω d·ªØ li·ªáu th·ª±c - KH√îNG t·∫°o sample data"""
        try:
            # ƒê∆∞·ªùng d·∫´n file d·ªØ li·ªáu th·ª±c
            data_paths = {
                'translated_data': 'data/translated_data.csv',
                'top2_clusters': 'data/top2_clusters_per_company.csv', 
                'sentiment_data': 'data/sentiment_by_company.csv'
            }
            
            # Ki·ªÉm tra file t·ªìn t·∫°i
            missing_files = []
            for name, path in data_paths.items():
                if not os.path.exists(path):
                    missing_files.append(path)
            
            if missing_files:
                st.markdown(f"""
                <div class="error-box">
                    <h4>‚ùå Kh√¥ng t√¨m th·∫•y c√°c file d·ªØ li·ªáu c·∫ßn thi·∫øt:</h4>
                    <ul>
                        {''.join([f'<li>{file}</li>' for file in missing_files])}
                    </ul>
                    <p><strong>H∆∞·ªõng d·∫´n:</strong></p>
                    <ol>
                        <li>T·∫°o th∆∞ m·ª•c <code>data/</code> trong project</li>
                        <li>Upload c√°c file CSV v√†o th∆∞ m·ª•c <code>data/</code></li>
                        <li>ƒê·∫£m b·∫£o t√™n file ch√≠nh x√°c nh∆∞ tr√™n</li>
                        <li>Refresh l·∫°i trang</li>
                    </ol>
                </div>
                """, unsafe_allow_html=True)
                return False
            
            # Load d·ªØ li·ªáu th·ª±c
            st.info("üìÇ ƒêang load d·ªØ li·ªáu th·ª±c t·ª´ files...")
            
            with st.spinner("Loading translated_data.csv..."):
                clustercty = pd.read_csv(data_paths['translated_data'])
                st.success(f"‚úÖ Loaded {len(clustercty)} companies from translated_data.csv")
            
            with st.spinner("Loading cluster data..."):
                new_data = pd.read_csv(data_paths['top2_clusters'])
                st.success(f"‚úÖ Loaded {len(new_data)} cluster records")
            
            with st.spinner("Loading sentiment data..."):
                sentiment_cln = pd.read_csv(data_paths['sentiment_data'])
                st.success(f"‚úÖ Loaded {len(sentiment_cln)} sentiment records")
            
            # Ki·ªÉm tra d·ªØ li·ªáu kh√¥ng tr·ªëng
            if clustercty.empty or new_data.empty or sentiment_cln.empty:
                st.error("‚ùå M·ªôt ho·∫∑c nhi·ªÅu file CSV tr·ªëng! Vui l√≤ng ki·ªÉm tra d·ªØ li·ªáu.")
                return False
            
            # Merge d·ªØ li·ªáu theo logic c·ªßa b·∫°n
            st.info("üîó ƒêang merge d·ªØ li·ªáu...")
            new_data = pd.merge(new_data, sentiment_cln[['Company Name','sentiment_group']], on='Company Name', how='left')
            clustercty = clustercty.merge(new_data[['Company Name', 'keyword', 'sentiment_group']], on='Company Name', how='left')
            
            # X·ª≠ l√Ω c·ªôt kh√¥ng c·∫ßn thi·∫øt
            if 'Unnamed: 0' in clustercty.columns:
                clustercty.drop(columns=['Unnamed: 0'], inplace=True)
            
            # ƒêi·ªÅn gi√° tr·ªã null
            clustercty['keyword'].fillna('kh√¥ng x√°c ƒë·ªãnh', inplace=True)
            clustercty['sentiment_group'].fillna('neutral', inplace=True)
            
            # Ki·ªÉm tra c√°c c·ªôt c·∫ßn thi·∫øt
            required_cols = _self.structured_cols + _self.text_cols
            missing_cols = [col for col in required_cols if col not in clustercty.columns]
            
            if missing_cols:
                st.error(f"‚ùå Thi·∫øu c√°c c·ªôt c·∫ßn thi·∫øt: {missing_cols}")
                st.info("üìã C√°c c·ªôt c√≥ s·∫µn: " + ", ".join(clustercty.columns.tolist()))
                return False
            
            # L∆∞u d·ªØ li·ªáu
            _self.clustercty = clustercty
            st.success(f"üéâ Load d·ªØ li·ªáu th√†nh c√¥ng: {len(clustercty)} c√¥ng ty v·ªõi ƒë·∫ßy ƒë·ªß th√¥ng tin!")
            
            # Hi·ªÉn th·ªã th·ªëng k√™
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üìä T·ªïng c√¥ng ty", len(clustercty))
            with col2:
                st.metric("üè≠ S·ªë ng√†nh", clustercty['Company industry'].nunique())
            with col3:
                st.metric("üòä C√≥ sentiment", clustercty['sentiment_group'].notna().sum())
            with col4:
                st.metric("üîë C√≥ keywords", clustercty['keyword'].notna().sum())
            
            return True
            
        except Exception as e:
            st.markdown(f"""
            <div class="error-box">
                <h4>‚ùå L·ªói load d·ªØ li·ªáu:</h4>
                <p><strong>Chi ti·∫øt l·ªói:</strong> {str(e)}</p>
                <p><strong>H∆∞·ªõng d·∫´n kh·∫Øc ph·ª•c:</strong></p>
                <ol>
                    <li>Ki·ªÉm tra format file CSV (UTF-8 encoding)</li>
                    <li>ƒê·∫£m b·∫£o c√°c c·ªôt c·∫ßn thi·∫øt c√≥ trong file</li>
                    <li>Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n file</li>
                    <li>Th·ª≠ load t·ª´ng file ri√™ng l·∫ª ƒë·ªÉ debug</li>
                </ol>
            </div>
            """, unsafe_allow_html=True)
            return False
    
    def prepare_features(self):
        """Chu·∫©n b·ªã features v·ªõi error handling t·ªët h∆°n"""
        try:
            if self.clustercty is None or self.clustercty.empty:
                st.error("‚ùå D·ªØ li·ªáu ch∆∞a ƒë∆∞·ª£c load ho·∫∑c tr·ªëng!")
                return False
            
            st.info("üîß ƒêang chu·∫©n b·ªã features...")
            
            # Ki·ªÉm tra c√°c c·ªôt text
            text_cols_available = [col for col in self.text_cols if col in self.clustercty.columns]
            if not text_cols_available:
                st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y c·ªôt text n√†o trong: {self.text_cols}")
                return False
            
            st.info(f"üìù S·ª≠ d·ª•ng c√°c c·ªôt text: {text_cols_available}")
            
            # X·ª≠ l√Ω vƒÉn b·∫£n v·ªõi text processor
            with st.spinner("ƒêang x·ª≠ l√Ω vƒÉn b·∫£n..."):
                self.clustercty['combined_text'] = self.clustercty[text_cols_available].fillna('').agg(' '.join, axis=1)
                self.clustercty['combined_text'] = self.clustercty['combined_text'].apply(self.text_processor.clean_pipeline)
            
            # Ki·ªÉm tra k·∫øt qu·∫£ x·ª≠ l√Ω text
            if self.clustercty['combined_text'].isna().all():
                st.error("‚ùå T·∫•t c·∫£ combined_text ƒë·ªÅu null sau khi x·ª≠ l√Ω!")
                return False
            
            # TF-IDF
            with st.spinner("ƒêang th·ª±c hi·ªán TF-IDF vectorization..."):
                tfidf_matrix = self.tfidf.fit_transform(self.clustercty['combined_text'])
                df_tfidf = pd.DataFrame(
                    tfidf_matrix.toarray(),
                    columns=self.tfidf.get_feature_names_out()
                )
            
            st.success(f"‚úÖ TF-IDF: {df_tfidf.shape[1]} features")
            
            # Ki·ªÉm tra c√°c c·ªôt structured
            structured_cols_available = [col for col in self.structured_cols if col in self.clustercty.columns]
            if not structured_cols_available:
                st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y c·ªôt structured n√†o trong: {self.structured_cols}")
                return False
            
            st.info(f"üèóÔ∏è S·ª≠ d·ª•ng c√°c c·ªôt structured: {structured_cols_available}")
            
            # One-hot encode
            with st.spinner("ƒêang th·ª±c hi·ªán One-hot encoding..."):
                self.df_structured_encoded = pd.get_dummies(self.clustercty[structured_cols_available], drop_first=True)
            
            st.success(f"‚úÖ One-hot encoding: {self.df_structured_encoded.shape[1]} features")
            
            # G·ªôp d·ªØ li·ªáu
            X_concat = pd.concat([
                self.df_structured_encoded.reset_index(drop=True), 
                df_tfidf.reset_index(drop=True)
            ], axis=1)
            
            st.info(f"üîó Combined features: {X_concat.shape[1]} total features")
            
            # PCA
            with st.spinner("ƒêang th·ª±c hi·ªán PCA dimensionality reduction..."):
                n_components = min(50, X_concat.shape[1] - 1, X_concat.shape[0] - 1)
                self.pca = PCA(n_components=n_components, random_state=42)
                self.X_full = self.pca.fit_transform(X_concat)
            
            st.success(f"‚úÖ PCA completed: {self.X_full.shape[1]} components")
            
            return True
            
        except Exception as e:
            st.error(f"‚ùå L·ªói chu·∫©n b·ªã features: {str(e)}")
            return False
    
    def find_optimal_clusters(self):
        """T√¨m s·ªë cluster t·ªëi ∆∞u"""
        K = range(2, min(11, len(self.clustercty)))
        silhouette_scores = []
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, k in enumerate(K):
            status_text.text(f'ƒêang test k={k}...')
            kmeans = KMeans(n_clusters=k, random_state=42)
            labels = kmeans.fit_predict(self.X_full)
            score = silhouette_score(self.X_full, labels)
            silhouette_scores.append(score)
            progress_bar.progress((i + 1) / len(K))
        
        status_text.empty()
        progress_bar.empty()
        
        best_k = K[silhouette_scores.index(max(silhouette_scores))]
        
        # Visualize silhouette scores
        fig = px.line(x=list(K), y=silhouette_scores, 
                     title="Silhouette Score vs Number of Clusters",
                     labels={'x': 'Number of Clusters (k)', 'y': 'Silhouette Score'})
        fig.add_vline(x=best_k, line_dash="dash", line_color="red", 
                     annotation_text=f"Best k={best_k}")
        st.plotly_chart(fig, use_container_width=True)
        
        return best_k, silhouette_scores
    
    def train_models(self):
        """Training c√°c m√¥ h√¨nh"""
        try:
            # T√¨m s·ªë cluster t·ªëi ∆∞u
            best_k, silhouette_scores = self.find_optimal_clusters()
        
            # Clustering v·ªõi k t·ªëi ∆∞u
            final_kmeans = KMeans(n_clusters=best_k, random_state=42)
            cluster_labels = final_kmeans.fit_predict(self.X_full)
            self.clustercty['cluster'] = cluster_labels
        
            # Chia train/test
            X_train, X_test, y_train, y_test = train_test_split(
                self.X_full, cluster_labels, test_size=0.2, random_state=42
            )
        
            # C√°c m√¥ h√¨nh
            models = {
                "Random Forest": RandomForestClassifier(n_estimators=50),
                "Logistic Regression": LogisticRegression(max_iter=500),
                "Naive Bayes": GaussianNB(),
                "Decision Tree": DecisionTreeClassifier(max_depth=10),
                "KNN": KNeighborsClassifier(n_neighbors=3)
            }
        
            results = []
            trained_models = {}
        
            progress_bar = st.progress(0)
            status_text = st.empty()
        
            for i, (name, model) in enumerate(models.items()):
                status_text.text(f'Training {name}...')
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                acc = accuracy_score(y_test, y_pred)
                results.append((name, acc))
                trained_models[name] = model
                progress_bar.progress((i + 1) / len(models))
        
            status_text.empty()
            progress_bar.empty()
            
            # Ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t
            best_model_name, best_acc = max(results, key=lambda x: x[1])
            self.best_model = trained_models[best_model_name]
        
            st.success(f"üèÜ M√¥ h√¨nh t·ªët nh·∫•t: {best_model_name} v·ªõi accuracy: {best_acc:.3f}")
        
            return results, best_model_name, best_acc, best_k
        
        except Exception as e:
            st.error(f"‚ùå L·ªói training models: {e}")
            return [], "", 0, 0
    
    def recommend_companies(self, user_input, text_input, threshold=0.1):
        """ƒê·ªÅ xu·∫•t c√¥ng ty"""
        try:
            # X·ª≠ l√Ω text input
            cleaned_text = self.text_processor.clean_pipeline(text_input)
            tfidf_vec = self.tfidf.transform([cleaned_text])
            
            # X·ª≠ l√Ω structured input
            structured_df = pd.DataFrame([user_input])
            structured_encoded = pd.get_dummies(structured_df)
            
            # ƒê·∫£m b·∫£o c√≥ ƒë·ªß columns
            missing_cols = set(self.df_structured_encoded.columns) - set(structured_encoded.columns)
            for col in missing_cols:
                structured_encoded[col] = 0
            structured_encoded = structured_encoded[self.df_structured_encoded.columns]
            
            # G·ªôp features
            user_input_vector = pd.concat([
                structured_encoded.reset_index(drop=True), 
                pd.DataFrame(tfidf_vec.toarray(), columns=self.tfidf.get_feature_names_out())
            ], axis=1)
            
            # PCA transform
            user_input_pca = self.pca.transform(user_input_vector)
            
            # Predict cluster
            predicted_cluster = self.best_model.predict(user_input_pca)[0]
            
            # T√≠nh Cosine Similarity
            company_text_vectors = self.tfidf.transform(self.clustercty['combined_text'])
            similarity_scores = cosine_similarity(tfidf_vec, company_text_vectors).flatten()
            self.clustercty['similarity_score'] = similarity_scores
            
            # L·ªçc c√¥ng ty
            matched = self.clustercty[
                (self.clustercty['cluster'] == predicted_cluster) & 
                (self.clustercty['similarity_score'] >= threshold)
            ].copy()
            
            matched = matched.sort_values(by='similarity_score', ascending=False).head(10)
            
            return matched, predicted_cluster
            
        except Exception as e:
            st.error(f"‚ùå L·ªói ƒë·ªÅ xu·∫•t: {e}")
            return pd.DataFrame(), -1
    
    def get_companies_by_cluster_sentiment(self, cluster_id=None, sentiment=None):
        """L·∫•y c√¥ng ty theo cluster v√† sentiment"""
        if self.clustercty is None:
            return pd.DataFrame()
        
        filtered_data = self.clustercty.copy()
        
        if cluster_id is not None:
            filtered_data = filtered_data[filtered_data['cluster'] == cluster_id]
        
        if sentiment is not None:
            filtered_data = filtered_data[filtered_data['sentiment_group'] == sentiment]
        
        return filtered_data

    def load_gensim_models(self):
        """Load pre-trained Gensim models"""
        try:
            # Gensim Problem 1
            if os.path.exists("gensim_dictionary.pkl"):
                with open("gensim_dictionary.pkl", "rb") as f:
                    self.gensim_dictionary = pickle.load(f)
                with open("gensim_corpus.pkl", "rb") as f:
                    self.gensim_corpus = pickle.load(f)
                self.gensim_tfidf = models.TfidfModel.load("gensim_tfidf_model.tfidf")
                self.gensim_index = similarities.SparseMatrixSimilarity.load("gensim_similarity.index")
                
                # Gensim Problem 2
                with open("gensim_dictionary_2.pkl", "rb") as f:
                    self.gensim_dictionary_2 = pickle.load(f)
                self.gensim_tfidf_2 = models.TfidfModel.load("gensim_tfidf_model_2.tfidf")
                self.gensim_index_2 = similarities.SparseMatrixSimilarity.load("gensim_similarity_2.index")
                
                return True
            else:
                st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Gensim models. Vui l√≤ng train models tr∆∞·ªõc.")
                return False
        except Exception as e:
            st.error(f"‚ùå L·ªói load Gensim models: {e}")
            return False

    def load_cosine_models(self):
        """Load pre-trained Cosine Similarity models"""
        try:
            if os.path.exists("cosine_index.pkl"):
                with open("cosine_index.pkl", "rb") as f:
                    self.cosine_index = pickle.load(f)
                with open("cosine_tfidf_2.pkl", "rb") as f:
                    self.cosine_tfidf_2 = pickle.load(f)
                with open("cosine_tfidf_matrix_2.pkl", "rb") as f:
                    self.cosine_tfidf_matrix_2 = pickle.load(f)
                return True
            else:
                st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Cosine models. Vui l√≤ng train models tr∆∞·ªõc.")
                return False
        except Exception as e:
            st.error(f"‚ùå L·ªói load Cosine models: {e}")
            return False

    def find_similar_companies_gensim(self, company_id, top_n=5):
        """T√¨m c√¥ng ty t∆∞∆°ng t·ª± b·∫±ng Gensim - Problem 1"""
        try:
            if not hasattr(self, 'gensim_tfidf'):
                st.error("‚ùå Gensim models ch∆∞a ƒë∆∞·ª£c load!")
                return pd.DataFrame(), []
            
            # L·∫•y vector TF-IDF c·ªßa c√¥ng ty ƒë∆∞·ª£c ch·ªçn
            tfidf_vec = self.gensim_tfidf[self.gensim_corpus[company_id]]
            
            # T√≠nh cosine similarity
            sims = self.gensim_index[tfidf_vec]
            
            # S·∫Øp x·∫øp v√† l·∫•y top N
            top_similar = sorted([(i, sim) for i, sim in enumerate(sims) if i != company_id], 
                               key=lambda x: x[1], reverse=True)[:top_n]
            
            # L·∫•y d·ªØ li·ªáu
            company_ids = [i[0] for i in top_similar]
            similarities = [round(i[1], 4) for i in top_similar]
            
            df_result = self.clustercty.iloc[company_ids].copy()
            df_result['similarity'] = similarities
            
            return df_result, top_similar
            
        except Exception as e:
            st.error(f"‚ùå L·ªói Gensim similarity: {e}")
            return pd.DataFrame(), []

    def search_companies_gensim(self, query_text, top_n=5):
        """T√¨m ki·∫øm c√¥ng ty b·∫±ng Gensim - Problem 2"""
        try:
            if not hasattr(self, 'gensim_tfidf_2'):
                st.error("‚ùå Gensim search models ch∆∞a ƒë∆∞·ª£c load!")
                return pd.DataFrame(), []
            
            # L√†m s·∫°ch v√† t√°ch t·ª´
            clean_text = self.text_processor.clean_pipeline(query_text)
            tokens = clean_text.split()
            
            # Chuy·ªÉn sang vector BoW
            bow_vector = self.gensim_dictionary_2.doc2bow(tokens)
            
            # Chuy·ªÉn sang TF-IDF
            tfidf_vector = self.gensim_tfidf_2[bow_vector]
            
            # T√≠nh ƒë·ªô t∆∞∆°ng t·ª±
            sims = self.gensim_index_2[tfidf_vector]
            
            # S·∫Øp x·∫øp v√† l·∫•y top N
            top_similar = sorted(enumerate(sims), key=lambda x: x[1], reverse=True)[:top_n]
            
            # L·∫•y d·ªØ li·ªáu
            company_ids = [i[0] for i in top_similar]
            similarities = [round(i[1], 4) for i in top_similar]
            
            df_result = self.clustercty.iloc[company_ids].copy()
            df_result['similarity'] = similarities
            
            return df_result, top_similar
            
        except Exception as e:
            st.error(f"‚ùå L·ªói Gensim search: {e}")
            return pd.DataFrame(), []

    def find_similar_companies_cosine(self, company_id, top_n=5):
        """T√¨m c√¥ng ty t∆∞∆°ng t·ª± b·∫±ng Cosine Similarity - Problem 1"""
        try:
            if not hasattr(self, 'cosine_index'):
                st.error("‚ùå Cosine models ch∆∞a ƒë∆∞·ª£c load!")
                return pd.DataFrame(), []
            
            # B·ªè ch√≠nh n√≥ ra
            sim_scores = self.cosine_index[company_id].copy()
            sim_scores[company_id] = -1
            
            # L·∫•y top N
            similar_indices = sim_scores.argsort()[-top_n:][::-1]
            
            # T·∫°o k·∫øt qu·∫£
            top_similar = [(i, sim_scores[i]) for i in similar_indices]
            df_result = self.clustercty.iloc[similar_indices].copy()
            df_result["similarity"] = [sim_scores[i] for i in similar_indices]
            
            return df_result, top_similar
            
        except Exception as e:
            st.error(f"‚ùå L·ªói Cosine similarity: {e}")
            return pd.DataFrame(), []

    def search_companies_cosine(self, query_text, top_n=5):
        """T√¨m ki·∫øm c√¥ng ty b·∫±ng Cosine Similarity - Problem 2"""
        try:
            if not hasattr(self, 'cosine_tfidf_2'):
                st.error("‚ùå Cosine search models ch∆∞a ƒë∆∞·ª£c load!")
                return pd.DataFrame(), []
            
            # L√†m s·∫°ch query
            cleaned_query = self.text_processor.clean_pipeline(query_text)
            
            # Chuy·ªÉn th√†nh vector TF-IDF
            query_vector = self.cosine_tfidf_2.transform([cleaned_query])
            
            # T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine
            sims = cosine_similarity(query_vector, self.cosine_tfidf_matrix_2)[0]
            
            # L·∫•y top N
            similar_indices = sims.argsort()[-top_n:][::-1]
            
            # T·∫°o k·∫øt qu·∫£
            top_similar = [(sims[i], i) for i in similar_indices]
            df_result = self.clustercty.iloc[similar_indices].copy()
            df_result["similarity_score"] = [sims[i] for i in similar_indices]
            
            return df_result, top_similar
            
        except Exception as e:
            st.error(f"‚ùå L·ªói Cosine search: {e}")
            return pd.DataFrame(), []

    def suggest_company_name(self):
        """T·∫°o selectbox cho vi·ªác ch·ªçn c√¥ng ty"""
        if self.clustercty is None or self.clustercty.empty:
            return None, None
        
        # T·∫°o mapping: t√™n c√¥ng ty ‚Üí index
        company_mapping = {}
        for idx, row in self.clustercty.iterrows():
            company_mapping[row['Company Name']] = idx
        
        # T·∫°o danh s√°ch t√™n c√¥ng ty
        company_list = sorted(company_mapping.keys())
        
        # T·∫°o selectbox
        selected_name = st.selectbox(
            "üè¢ Ch·ªçn c√¥ng ty:",
            options=company_list,
            key="company_selector"
        )
        
        # L·∫•y index t∆∞∆°ng ·ª©ng
        selected_id = company_mapping.get(selected_name, None)
        return selected_name, selected_id

    def show_company_detail(self, data, title=None, expanded=False):
        """Hi·ªÉn th·ªã chi ti·∫øt c√¥ng ty"""
        with st.expander(title or f"{data['Company Name']}", expanded=expanded):
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown(f"**üè≠ Ng√†nh:** {data.get('Company industry', 'N/A')}")
                st.markdown(f"**üë• Quy m√¥:** {data.get('Company size', 'N/A')}")
                st.markdown(f"**üåç Qu·ªëc gia:** {data.get('Country', 'N/A')}")
                st.markdown(f"**üè¢ Lo·∫°i:** {data.get('Company Type', 'N/A')}")
            
            with col2:
                st.markdown(f"**üìÖ L√†m vi·ªác:** {data.get('Working days', 'N/A')}")
                st.markdown(f"**‚è∞ OT Policy:** {data.get('Overtime Policy', 'N/A')}")
                st.markdown(f"**üòä Sentiment:** {data.get('sentiment_group', 'N/A')}")
                st.markdown(f"**üîë Keywords:** {data.get('keyword', 'N/A')}")
            
            if 'Company overview_new' in data:
                st.markdown("**üìù M√¥ t·∫£ c√¥ng ty:**")
                st.write(data.get('Company overview_new', 'Kh√¥ng c√≥ th√¥ng tin'))
            
            if "Why you'll love working here_new" in data:
                st.markdown("**üíù T·∫°i sao b·∫°n s·∫Ω y√™u th√≠ch:**")
                st.write(data.get("Why you'll love working here_new", 'Kh√¥ng c√≥ th√¥ng tin'))
            
            if 'Our key skills_new' in data:
                st.markdown("**üîß K·ªπ nƒÉng c·∫ßn thi·∫øt:**")
                st.write(data.get('Our key skills_new', 'Kh√¥ng c√≥ th√¥ng tin'))

import subprocess
class PySparkMLSystem:
    def __init__(self):
        self.spark = None
        self.spark_df_ml = None
        self.pyspark_results = {}

    def initialize_spark(self):
        """Kh·ªüi t·∫°o Spark Session v·ªõi ki·ªÉm tra Java"""
        try:
            # Ki·ªÉm tra java -version
            result = subprocess.run(['java', '-version'], capture_output=True, text=True)
            if result.returncode != 0:
                st.error("‚ùå Java kh√¥ng ƒë∆∞·ª£c c√†i ƒë·∫∑t ho·∫∑c kh√¥ng trong PATH")
                return False
        except FileNotFoundError:
            st.error("‚ùå Java kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y. Vui l√≤ng c√†i ƒë·∫∑t Java 8 ho·∫∑c 11.")
            return False

        try:
            # T·∫°o SparkSession an to√†n
            self.spark = SparkSession.builder \
                .appName("CompanyRecommendation") \
                .config("spark.driver.memory", "2g") \
                .config("spark.executor.memory", "2g") \
                .config("spark.sql.adaptive.enabled", "true") \
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
                .getOrCreate()

            # Test Spark b·∫±ng c√°ch t·∫°o DataFrame
            test_df = self.spark.createDataFrame([(1, "test")], ["id", "value"])
            test_df.count()  # √©p th·ª±c thi

            self.spark.sparkContext.setLogLevel("ERROR")
            st.success("‚úÖ Spark ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng!")
            return True

        except Exception as e:
            st.error(f"‚ùå L·ªói kh·ªüi t·∫°o Spark: {e}")
            return False

    
    def prepare_spark_data(self, X_concat, cluster_labels):
        """Chu·∫©n b·ªã d·ªØ li·ªáu cho PySpark"""
        try:
            # Th√™m cluster labels v√†o X_concat
            X_concat_with_labels = X_concat.copy()
            X_concat_with_labels['cluster'] = cluster_labels
            
            # Convert to Spark DataFrame
            self.spark_df_ml = self.spark.createDataFrame(X_concat_with_labels)
            
            # Assemble features
            feature_columns = X_concat.columns.tolist()
            assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")
            self.spark_df_ml = assembler.transform(self.spark_df_ml)
            
            # Select features and labels
            self.spark_df_ml = self.spark_df_ml.select("features", "cluster")
            
            return True
            
        except Exception as e:
            st.error(f"‚ùå L·ªói chu·∫©n b·ªã d·ªØ li·ªáu Spark: {e}")
            return False
    
    def train_pyspark_models(self):
        """Training PySpark models"""
        try:
            # Split data
            train_data, test_data = self.spark_df_ml.randomSplit([0.8, 0.2], seed=42)
            
            # Evaluator
            evaluator = MulticlassClassificationEvaluator(
                labelCol="cluster", 
                predictionCol="prediction", 
                metricName="accuracy"
            )
            
            # Logistic Regression
            lr = SparkLogisticRegression(featuresCol="features", labelCol="cluster")
            lr_model = lr.fit(train_data)
            lr_predictions = lr_model.transform(test_data)
            lr_accuracy = evaluator.evaluate(lr_predictions)
            
            # Decision Tree
            dt = SparkDecisionTreeClassifier(featuresCol="features", labelCol="cluster")
            dt_model = dt.fit(train_data)
            dt_predictions = dt_model.transform(test_data)
            dt_accuracy = evaluator.evaluate(dt_predictions)
            
            self.pyspark_results = {
                "PySpark Logistic Regression": lr_accuracy,
                "PySpark Decision Tree": dt_accuracy
            }
            
            return self.pyspark_results
            
        except Exception as e:
            st.error(f"‚ùå L·ªói training PySpark models: {e}")
            return {}
    
    def stop_spark(self):
        """D·ª´ng Spark Session"""
        if self.spark:
            self.spark.stop()

# Sidebar
st.sidebar.markdown("## ü§ñ AI Company Recommendation System")
st.sidebar.markdown("---")

# Menu categories
menu_options = ["Business Objective", "Company Suggest", "Build Project", "New Prediction"]
selected_menu = st.sidebar.selectbox("üìã Select Category:", menu_options)

# Th√¥ng tin ng∆∞·ªùi t·∫°o
st.sidebar.markdown("---")
st.sidebar.markdown("### üë• Creators Information")
st.sidebar.markdown("**Creator 1:**")
st.sidebar.markdown("üìß V√µ Minh Tr√≠")
st.sidebar.markdown("‚úâÔ∏è trivm203@gmail.com")
st.sidebar.markdown("**Creator 2:**") 
st.sidebar.markdown("üìß Ph·∫°m Th·ªã Thu Th·∫£o")
st.sidebar.markdown("‚úâÔ∏è phamthithuthao@email.com")

# Main content
if selected_menu == "Business Objective":
    st.markdown('<h1 class="main-header">üéØ Business Objective</h1>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="section-header">M·ª•c ti√™u c·ªßa d·ª± √°n</div>
    
    H·ªá th·ªëng ƒë·ªÅ xu·∫•t c√¥ng ty n√†y ƒë∆∞·ª£c ph√°t tri·ªÉn nh·∫±m:
    
    ### üéØ M·ª•c ti√™u ch√≠nh:
    - **H·ªó tr·ª£ ·ª©ng vi√™n**: Gi√∫p ·ª©ng vi√™n t√¨m ki·∫øm c√¥ng ty ph√π h·ª£p v·ªõi mong mu·ªën v√† k·ªπ nƒÉng c·ªßa h·ªç
    - **T·ªëi ∆∞u h√≥a tuy·ªÉn d·ª•ng**: C·∫£i thi·ªán qu√° tr√¨nh matching gi·ªØa ·ª©ng vi√™n v√† nh√† tuy·ªÉn d·ª•ng
    - **Ph√¢n t√≠ch th·ªã tr∆∞·ªùng**: Cung c·∫•p insights v·ªÅ xu h∆∞·ªõng tuy·ªÉn d·ª•ng v√† y√™u c·∫ßu c√¥ng vi·ªác
    
    ### üîç T√≠nh nƒÉng ch√≠nh:
    1. **Ph√¢n t√≠ch c√¥ng ty**: Cung c·∫•p th√¥ng tin chi ti·∫øt v·ªÅ c√°c c√¥ng ty h√†ng ƒë·∫ßu
    2. **ƒê·ªÅ xu·∫•t th√¥ng minh**: S·ª≠ d·ª•ng machine learning ƒë·ªÉ ƒë·ªÅ xu·∫•t c√¥ng ty ph√π h·ª£p
    3. **So s√°nh c√¥ng ty**: Gi√∫p ·ª©ng vi√™n so s√°nh c√°c l·ª±a ch·ªçn kh√°c nhau
    4. **D·ª± ƒëo√°n xu h∆∞·ªõng**: Ph√¢n t√≠ch v√† d·ª± ƒëo√°n xu h∆∞·ªõng tuy·ªÉn d·ª•ng
    
    ### üìä L·ª£i √≠ch:
    - Ti·∫øt ki·ªám th·ªùi gian t√¨m ki·∫øm vi·ªác l√†m
    - TƒÉng t·ª∑ l·ªá match th√†nh c√¥ng
    - Cung c·∫•p th√¥ng tin minh b·∫°ch v·ªÅ th·ªã tr∆∞·ªùng lao ƒë·ªông
    - H·ªó tr·ª£ quy·∫øt ƒë·ªãnh ngh·ªÅ nghi·ªáp
    
    ### ü§ñ AI Features:
    - **Vietnamese Text Processing**: X·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát v·ªõi teencode, stopwords
    - **TF-IDF Vectorization**: Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh vector s·ªë
    - **K-means Clustering**: Ph√¢n nh√≥m c√¥ng ty theo ƒë·∫∑c ƒëi·ªÉm
    - **Multiple ML Models**: Random Forest, SVM, Logistic Regression, etc.
    - **Cosine Similarity**: T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng th√¥ng minh
    """, unsafe_allow_html=True)

elif selected_menu == "Company Suggest":
    st.markdown('<h1 class="main-header">üè¢ Company Suggestions</h1>', unsafe_allow_html=True)

    # Load d·ªØ li·ªáu n·∫øu ch∆∞a load
    if not st.session_state.data_loaded:
        with st.spinner("üîÑ ƒêang load d·ªØ li·ªáu..."):
            ml_system = CompanyRecommendationSystem()
            if ml_system.load_data():
                ml_system.load_gensim_models()
                ml_system.load_cosine_models()
                st.session_state.ml_system = ml_system
                st.session_state.data_loaded = True
            else:
                st.stop()

    ml_system = st.session_state.ml_system

    tab1, tab2, tab3 = st.tabs(["üß† Gensim Recommendation", "üìä Cosine Similarity", "üîç Smart Text Search"])

    # ---------------------- TAB 1: Gensim Recommendation ---------------------- #
    with tab1:
        st.markdown("### üß† ƒê·ªÅ xu·∫•t c√¥ng ty b·∫±ng Gensim")

        selected_company = st.selectbox("üè¢ Ch·ªçn c√¥ng ty:", ml_system.clustercty['Company Name'].unique())
        top_n = st.slider("üî¢ S·ªë c√¥ng ty ƒë·ªÅ xu·∫•t:", 1, 10, 5)

        if st.button("üîç T√¨m c√¥ng ty t∆∞∆°ng t·ª± (Gensim)", key="gensim_button"):
            st.markdown("---")
            selected_id = ml_system.clustercty[ml_system.clustercty['Company Name'] == selected_company].index[0]

            st.markdown("### üè¢ C√¥ng ty ƒë∆∞·ª£c ch·ªçn:")
            ml_system.show_company_detail(ml_system.clustercty.loc[selected_id], expanded=True)

            df_result, _ = ml_system.find_similar_companies_gensim(selected_id, top_n)
            if not df_result.empty:
                st.markdown("### ü§ù C√°c c√¥ng ty t∆∞∆°ng t·ª±:")
                for idx, row in df_result.iterrows():
                    ml_system.show_company_detail(row, title=f"#{idx+1} {row['Company Name']} - Similarity: {row['similarity']:.3f}")
            else:
                st.info("‚ùå Kh√¥ng t√¨m th·∫•y c√¥ng ty t∆∞∆°ng t·ª±.")

    # ---------------------- TAB 2: Cosine Similarity Search ---------------------- #
    with tab2:
        st.markdown("### üìä T√¨m c√¥ng ty t∆∞∆°ng t·ª± b·∫±ng m√¥ t·∫£ (Cosine Similarity)")

        query_text = st.text_area("üìù Nh·∫≠p m√¥ t·∫£ c√¥ng ty mong mu·ªën:", placeholder="V√≠ d·ª•: m√¥i tr∆∞·ªùng nƒÉng ƒë·ªông, c√¥ng ngh·ªá m·ªõi, AI, machine learning...")
        top_n_cosine = st.slider("üî¢ S·ªë c√¥ng ty t∆∞∆°ng t·ª±:", 1, 10, 5)

        if st.button("üîç T√¨m theo m√¥ t·∫£ (Cosine)", key="cosine_button"):
            if query_text.strip():
                df_result, _ = ml_system.search_companies_cosine(query_text, top_n_cosine)

                if not df_result.empty:
                    st.markdown("### üîé K·∫øt qu·∫£ t∆∞∆°ng t·ª±:")
                    for idx, row in df_result.iterrows():
                        ml_system.show_company_detail(row, title=f"#{idx+1} {row['Company Name']} - Similarity: {row['similarity_score']:.3f}")
                else:
                    st.warning("‚ùå Kh√¥ng t√¨m th·∫•y c√¥ng ty ph√π h·ª£p.")

    with tab3:
        st.markdown("### üîç T√¨m c√¥ng ty theo t·ª´ kh√≥a")

        keyword_query = st.text_input("T√¨m theo t·ª´ kh√≥a:", placeholder="V√≠ d·ª•: fintech, AI, c√¥ng ngh·ªá, l∆∞∆°ng cao...")

        if keyword_query.strip():
            cleaned_query = ml_system.text_processor.clean_pipeline(keyword_query)
            query_words = set(cleaned_query.split())

            search_results = ml_system.clustercty.copy()
            search_results['search_score'] = 0

            for idx, row in search_results.iterrows():
                combined_text = ' '.join([
                    row.get('Company overview_new', ''),
                    row.get('Our key skills_new', ''),
                    row.get("Why you'll love working here_new", ''),
                    row.get('keyword', '')
                ])
                cleaned = ml_system.text_processor.clean_pipeline(combined_text)
                words = set(cleaned.split())
                search_results.at[idx, 'search_score'] = len(words.intersection(query_words)) / len(query_words)

            top_matches = search_results[search_results['search_score'] > 0]
            top_matches = top_matches.sort_values(by='search_score', ascending=False).head(10)

            if not top_matches.empty:
                st.markdown("### ‚úÖ K·∫øt qu·∫£ t√¨m th·∫•y:")
                for idx, row in top_matches.iterrows():
                    ml_system.show_company_detail(row, title=f"#{idx+1} {row['Company Name']} - Score: {row['search_score']:.3f}")
            else:
                st.warning("Kh√¥ng t√¨m th·∫•y c√¥ng ty n√†o ch·ª©a t·ª´ kh√≥a ph√π h·ª£p.")

elif selected_menu == "Build Project":
    st.markdown('<h1 class="main-header">üî® Build Project</h1>', unsafe_allow_html=True)
    
    # Kh·ªüi t·∫°o v√† load d·ªØ li·ªáu
    if not st.session_state.data_loaded:
        with st.spinner("üîÑ ƒêang load d·ªØ li·ªáu th·ª±c..."):
            ml_system = CompanyRecommendationSystem()
            if ml_system.load_data():
                st.session_state.ml_system = ml_system
                st.session_state.data_loaded = True
            else:
                st.stop()
    
    # Training options
    st.markdown("### üöÄ Training Machine Learning Models:")
    
    sklearn_training = st.button("üî¨ Train Sklearn Models", use_container_width=True)
    
    if PYSPARK_AVAILABLE:
        pyspark_training = st.button("‚ö° Train PySpark Models", use_container_width=True)
    
    else:
        st.info("üí° **PySpark kh√¥ng kh·∫£ d·ª•ng** - Ch·ªâ s·ª≠ d·ª•ng Sklearn Models")
    
    # Sklearn Training
    if sklearn_training and st.session_state.data_loaded:
        if not st.session_state.model_trained:
            with st.spinner("ü§ñ ƒêang training sklearn models..."):
                ml_system = st.session_state.ml_system
                
                if ml_system.prepare_features():
                    results, best_model_name, best_acc, best_k = ml_system.train_models()
                    
                    if results:
                        st.session_state.model_trained = True
                        st.session_state.training_results = results
                        st.session_state.best_model_name = best_model_name
                        st.session_state.best_acc = best_acc
                        st.session_state.best_k = best_k
                        st.success("‚úÖ Sklearn Training ho√†n t·∫•t!")
                        st.rerun()
    
    # PySpark Training
    if PYSPARK_AVAILABLE and 'pyspark_training' in locals() and pyspark_training:
        if not st.session_state.get('pyspark_trained', False):
            if not st.session_state.model_trained:
                st.warning("‚ö†Ô∏è Vui l√≤ng train sklearn models tr∆∞·ªõc!")
            else:
                with st.spinner("‚ö° ƒêang training PySpark models..."):
                    try:
                        ml_system = st.session_state.ml_system
                        pyspark_system = PySparkMLSystem()
                        
                        if pyspark_system.initialize_spark():
                            # Chu·∫©n b·ªã d·ªØ li·ªáu
                            X_concat = pd.concat([
                                ml_system.df_structured_encoded.reset_index(drop=True),
                                pd.DataFrame(
                                    ml_system.tfidf.transform(ml_system.clustercty['combined_text']).toarray(),
                                    columns=ml_system.tfidf.get_feature_names_out()
                                )
                            ], axis=1)
                            
                            cluster_labels = ml_system.clustercty['cluster'].values
                            
                            if pyspark_system.prepare_spark_data(X_concat, cluster_labels):
                                pyspark_results = pyspark_system.train_pyspark_models()
                                
                                if pyspark_results:
                                    st.session_state.pyspark_trained = True
                                    st.session_state.pyspark_results = pyspark_results
                                    st.success("‚úÖ PySpark Training ho√†n t·∫•t!")
                                    st.rerun()
                            
                            pyspark_system.stop_spark()
                        
                    except Exception as e:
                        st.error(f"‚ùå L·ªói PySpark training: {e}")
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    if st.session_state.get('model_trained', False):
        st.markdown("### üìä K·∫øt qu·∫£ Training")
        
        ml_system = st.session_state.ml_system
        
        # Metrics overview
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("üìä S·ªë c√¥ng ty", len(ml_system.clustercty))
        with col2:
            st.metric("üéØ S·ªë clusters", st.session_state.best_k)
        with col3:
            st.metric("üèÜ Best Model", st.session_state.best_model_name)
        with col4:
            st.metric("üìà Best Accuracy", f"{st.session_state.best_acc:.3f}")
        
        # Sklearn Results
        st.markdown("#### üî¨ Sklearn Models Results:")
        sklearn_results_df = pd.DataFrame(st.session_state.training_results, columns=['M√¥ h√¨nh', 'Accuracy'])
        sklearn_results_df['Accuracy (%)'] = (sklearn_results_df['Accuracy'] * 100).round(2)
        sklearn_results_df['Framework'] = 'Sklearn'
        
        st.dataframe(sklearn_results_df[['M√¥ h√¨nh', 'Accuracy (%)', 'Framework']], use_container_width=True)
        
        # PySpark Results (if available)
        if st.session_state.get('pyspark_trained', False):
            st.markdown("#### ‚ö° PySpark Models Results:")
            pyspark_results = st.session_state.pyspark_results
            pyspark_results_df = pd.DataFrame(list(pyspark_results.items()), columns=['M√¥ h√¨nh', 'Accuracy'])
            pyspark_results_df['Accuracy (%)'] = (pyspark_results_df['Accuracy'] * 100).round(2)
            pyspark_results_df['Framework'] = 'PySpark'
            
            st.dataframe(pyspark_results_df[['M√¥ h√¨nh', 'Accuracy (%)', 'Framework']], use_container_width=True)
            
            # Combined comparison
            st.markdown("#### üÜö Sklearn vs PySpark Comparison:")
            combined_results = pd.concat([
                sklearn_results_df[['M√¥ h√¨nh', 'Accuracy (%)', 'Framework']],
                pyspark_results_df[['M√¥ h√¨nh', 'Accuracy (%)', 'Framework']]
            ], ignore_index=True)
            
            fig_comparison = px.bar(
                combined_results, 
                x='M√¥ h√¨nh', 
                y='Accuracy (%)',
                color='Framework',
                title="Sklearn vs PySpark Models Comparison",
                barmode='group'
            )
            fig_comparison.update_layout(xaxis_tickangle=-45)
            st.plotly_chart(fig_comparison, use_container_width=True)
        
        # Sklearn visualization
        fig_sklearn = px.bar(
            sklearn_results_df, 
            x='M√¥ h√¨nh', 
            y='Accuracy (%)',
            title="Sklearn Models Performance",
            color='Accuracy (%)',
            color_continuous_scale='viridis'
        )
        fig_sklearn.update_layout(xaxis_tickangle=-45)
        st.plotly_chart(fig_sklearn, use_container_width=True)
        
        # Cluster analysis
        st.markdown("#### üéØ Cluster Analysis:")
        if 'cluster' in ml_system.clustercty.columns:
            cluster_stats = ml_system.clustercty.groupby('cluster').agg({
                'Company Name': 'count',
                'Company industry': lambda x: x.mode().iloc[0] if not x.mode().empty else 'N/A',
                'sentiment_group': lambda x: x.mode().iloc[0] if not x.mode().empty else 'N/A'
            })
            cluster_stats.columns = ['S·ªë c√¥ng ty', 'Ng√†nh ch·ªß ƒë·∫°o', 'Sentiment ch·ªß ƒë·∫°o']
            st.dataframe(cluster_stats, use_container_width=True)
            
            # Cluster distribution
            fig_cluster = px.pie(
                values=cluster_stats['S·ªë c√¥ng ty'],
                names=cluster_stats.index,
                title="Ph√¢n b·ªë c√¥ng ty theo Cluster"
            )
            st.plotly_chart(fig_cluster, use_container_width=True)

elif selected_menu == "New Prediction":
    st.markdown('<h1 class="main-header">üîÆ New Prediction</h1>', unsafe_allow_html=True)
    
    if not st.session_state.get('model_trained', False):
        st.warning("‚ö†Ô∏è Vui l√≤ng train model ·ªü m·ª•c 'Build Project' tr∆∞·ªõc khi s·ª≠ d·ª•ng t√≠nh nƒÉng n√†y!")
        st.stop()
    
    ml_system = st.session_state.ml_system
    
    st.markdown("### üéØ Nh·∫≠p th√¥ng tin ƒë·ªÉ nh·∫≠n ƒë·ªÅ xu·∫•t c√¥ng ty ph√π h·ª£p")
    
    # User input form
    col1, col2 = st.columns(2)
    
    with col1:
        company_type = st.selectbox("üè¢ Lo·∫°i c√¥ng ty:", ml_system.clustercty['Company Type'].unique())
        company_industry = st.selectbox("üè≠ Ng√†nh ngh·ªÅ:", ml_system.clustercty['Company industry'].unique())
        company_size = st.selectbox("üë• Quy m√¥ c√¥ng ty:", ml_system.clustercty['Company size'].unique())
    
    with col2:
        country = st.selectbox("üåç Qu·ªëc gia:", ml_system.clustercty['Country'].unique())
        working_days = st.selectbox("üìÖ Ng√†y l√†m vi·ªác:", ml_system.clustercty['Working days'].unique())
        overtime_policy = st.selectbox("‚è∞ Ch√≠nh s√°ch OT:", ml_system.clustercty['Overtime Policy'].unique())
    
    # Text input
    text_input = st.text_area(
        "üìù M√¥ t·∫£ mong mu·ªën c·ªßa b·∫°n:",
        placeholder="V√≠ d·ª•: T√¥i mu·ªën l√†m vi·ªác trong m√¥i tr∆∞·ªùng c√¥ng ngh·ªá, s·ª≠ d·ª•ng AI v√† machine learning, c√≥ c∆° h·ªôi ph√°t tri·ªÉn...",
        height=100
    )
    
    # Similarity threshold
    threshold = st.slider("üéØ Ng∆∞·ª°ng ƒë·ªô t∆∞∆°ng ƒë·ªìng:", 0.0, 1.0, 0.1, 0.05)
    
    if st.button("üîÆ D·ª± ƒëo√°n v√† ƒê·ªÅ xu·∫•t", use_container_width=True):
        if text_input.strip():
            with st.spinner("ü§ñ AI ƒëang ph√¢n t√≠ch v√† ƒë·ªÅ xu·∫•t..."):
                # Chu·∫©n b·ªã input
                user_input = {
                    'Company Type': company_type,
                    'Company industry': company_industry,
                    'Company size': company_size,
                    'Country': country,
                    'Working days': working_days,
                    'Overtime Policy': overtime_policy
                }
                
                # L·∫•y ƒë·ªÅ xu·∫•t
                matched_companies, predicted_cluster = ml_system.recommend_companies(
                    user_input, text_input, threshold
                )
                
                if not matched_companies.empty:
                    # Hi·ªÉn th·ªã k·∫øt qu·∫£ prediction
                    st.markdown(f"""
                    <div class="prediction-container">
                        <h3>üéØ K·∫øt qu·∫£ D·ª± ƒëo√°n</h3>
                        <p><strong>Cluster ƒë∆∞·ª£c d·ª± ƒëo√°n:</strong> {predicted_cluster}</p>
                        <p><strong>S·ªë c√¥ng ty ph√π h·ª£p:</strong> {len(matched_companies)}</p>
                        <p><strong>Ng∆∞·ª°ng t∆∞∆°ng ƒë·ªìng:</strong> {threshold}</p>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    st.markdown("### üèÜ Top c√¥ng ty ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t:")
                    
                    # Hi·ªÉn th·ªã t·ª´ng c√¥ng ty
                    for idx, (_, company) in enumerate(matched_companies.iterrows(), 1):
                        similarity = company['similarity_score']
                        
                        with st.expander(f"#{idx} üè¢ {company['Company Name']} - Similarity: {similarity:.3f}"):
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.markdown(f"**üè≠ Ng√†nh:** {company.get('Company industry', 'N/A')}")
                                st.markdown(f"**üë• Quy m√¥:** {company.get('Company size', 'N/A')}")
                                st.markdown(f"**üåç Qu·ªëc gia:** {company.get('Country', 'N/A')}")
                                st.markdown(f"**üè¢ Lo·∫°i:** {company.get('Company Type', 'N/A')}")
                            
                            with col2:
                                st.markdown(f"**üìÖ L√†m vi·ªác:** {company.get('Working days', 'N/A')}")
                                st.markdown(f"**‚è∞ OT Policy:** {company.get('Overtime Policy', 'N/A')}")
                                st.markdown(f"**üòä Sentiment:** {company.get('sentiment_group', 'N/A')}")
                                st.markdown(f"**üîë Keywords:** {company.get('keyword', 'N/A')}")
                            
                            # Similarity score highlight
                            st.markdown(f"""
                            <div class="similarity-score">
                                üéØ ƒê·ªô t∆∞∆°ng ƒë·ªìng: {similarity:.3f} ({similarity*100:.1f}%)
                            </div>
                            """, unsafe_allow_html=True)
                            
                            # Company details
                            if 'Company overview_new' in company:
                                st.markdown("**üìù M√¥ t·∫£ c√¥ng ty:**")
                                st.write(company.get('Company overview_new', 'Kh√¥ng c√≥ th√¥ng tin'))
                            
                            if "Why you'll love working here_new" in company:
                                st.markdown("**üíù T·∫°i sao b·∫°n s·∫Ω y√™u th√≠ch:**")
                                st.write(company.get("Why you'll love working here_new", 'Kh√¥ng c√≥ th√¥ng tin'))
                            
                            if 'Our key skills_new' in company:
                                st.markdown("**üîß K·ªπ nƒÉng c·∫ßn thi·∫øt:**")
                                st.write(company.get('Our key skills_new', 'Kh√¥ng c√≥ th√¥ng tin'))
                    
                    # Visualization
                    if len(matched_companies) > 1:
                        st.markdown("### üìä Bi·ªÉu ƒë·ªì ƒë·ªô t∆∞∆°ng ƒë·ªìng:")
                        fig_similarity = px.bar(
                            x=matched_companies['Company Name'],
                            y=matched_companies['similarity_score'],
                            title="ƒê·ªô t∆∞∆°ng ƒë·ªìng c·ªßa c√°c c√¥ng ty ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t",
                            labels={'x': 'C√¥ng ty', 'y': 'ƒê·ªô t∆∞∆°ng ƒë·ªìng'}
                        )
                        fig_similarity.update_layout(xaxis_tickangle=-45)
                        st.plotly_chart(fig_similarity, use_container_width=True)
                
                else:
                    st.warning(f"‚ùå Kh√¥ng t√¨m th·∫•y c√¥ng ty ph√π h·ª£p v·ªõi ng∆∞·ª°ng t∆∞∆°ng ƒë·ªìng {threshold}")
                    st.info("üí° **G·ª£i √Ω:** Th·ª≠ gi·∫£m ng∆∞·ª°ng t∆∞∆°ng ƒë·ªìng ho·∫∑c thay ƒë·ªïi m√¥ t·∫£ mong mu·ªën")
        else:
            st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p m√¥ t·∫£ mong mu·ªën c·ªßa b·∫°n!")



